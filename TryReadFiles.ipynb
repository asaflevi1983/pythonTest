{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 13, 301, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__\n",
    "import csv\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import pandas as pd\n",
    "from time import gmtime, strftime\n",
    "\n",
    "\n",
    "csvfile=open('C:/git/cropped/result.csv');\n",
    "reader = csv.DictReader(csvfile)\n",
    "\n",
    "image_array  = np.empty([])\n",
    "image_labels=np.empty((1))\n",
    "\n",
    "img = image.load_img('C:/git/cropped/sample1.jpg',grayscale=True)  # this is a PIL image\n",
    "image_array_size = image.img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "image_array_size= image_array_size.reshape((1,) + image_array_size.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "image_array=np.empty(image_array_size.shape)\n",
    "image_array_size.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.4'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function ndarray.dump>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_array.dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(reader)\n",
    "\n",
    "img=image.load_img(row['filePath'],grayscale=True)\n",
    "x_value=row['x']\n",
    "image_array[0]= image.img_to_array(img)\n",
    "image_labels[0]=x_value\n",
    "\n",
    "next(reader)\n",
    "\n",
    "\n",
    "for row in reader:\n",
    "      #  print(row['x'], row['y'],row['filePath'])\n",
    "        img=image.load_img(row['filePath'],grayscale=True)\n",
    "        x_value=row['x']\n",
    "        image_array= np.append(image_array,[image.img_to_array(img)],axis=0)\n",
    "        image_labels=np.append(image_labels,x_value)\n",
    "        #x = np.append(x, )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(473, 13, 301, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.squeeze(image_array, axis=3).shape\n",
    "image_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(473,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'154.888229'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x191000ea7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.squeeze(image_array[0], axis=2))\n",
    "image_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'107.566246'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x191000cd6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.squeeze(image_array[4], axis=2))\n",
    "image_labels[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'160.122116'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1910002d438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.squeeze(image_array[5], axis=2))\n",
    "image_labels[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'117.07312'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19100199748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.squeeze(image_array[6], axis=2))\n",
    "image_labels[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 301, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_array[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'152.385468'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_labels[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['154.888229', '207.192993', '99.226677', '152.385468', '107.566246',\n",
       "       '160.122116', '117.07312', '142.775421', '106.087975', '136.91156',\n",
       "       '115.228539', '155.422791', '196.123276', '102.695328', '155.43663',\n",
       "       '103.915276', '134.6707', '109.622116', '122.580284', '110.657669',\n",
       "       '148.667923', '110.997406', '138.187805', '194.741257',\n",
       "       '116.708481', '153.688629', '109.461647', '159.791092', '99.025284',\n",
       "       '149.253143', '112.981071', '152.73642', '113.814461', '154.775711',\n",
       "       '154.024506', '119.933556', '150.810364', '106.79084', '151.905685',\n",
       "       '109.834557', '155.275299', '119.893448', '163.030563',\n",
       "       '110.753433', '162.431702', '113.085526', '107.304756',\n",
       "       '136.944763', '101.227585', '156.257278', '113.845451',\n",
       "       '149.071854', '112.559135', '161.975174', '119.5289', '116.918449',\n",
       "       '113.496269', '106.418221', '143.299026', '112.592712',\n",
       "       '159.796967', '122.819305', '142.271149', '126.278687',\n",
       "       '134.827942', '112.508675', '140.569519', '150.788742',\n",
       "       '103.932724', '152.390396', '127.402061', '156.969254',\n",
       "       '108.086983', '150.163681', '128.414688', '160.501678',\n",
       "       '114.126945', '152.103088', '126.168449', '113.485596', '160.20874',\n",
       "       '104.069572', '143.551559', '118.230461', '147.895111',\n",
       "       '123.896339', '148.661209', '115.170509', '160.108475',\n",
       "       '193.203476', '120.098717', '157.94548', '112.78447', '148.630539',\n",
       "       '117.561653', '110.304916', '142.263046', '111.001099',\n",
       "       '128.612152', '108.842636', '218.006729', '131.15126', '111.854416',\n",
       "       '117.055923', '131.180832', '114.533241', '153.970871',\n",
       "       '115.907433', '142.126953', '107.685333', '157.979538',\n",
       "       '141.478348', '219.901962', '134.224533', '132.074417',\n",
       "       '115.421509', '159.910156', '123.023697', '151.62236', '106.201797',\n",
       "       '148.096512', '117.821945', '137.095123', '200.170364',\n",
       "       '106.964043', '158.776459', '115.316208', '154.033157',\n",
       "       '111.735962', '149.585541', '122.285789', '151.512543',\n",
       "       '111.803528', '160.198532', '180.922379', '121.859787',\n",
       "       '149.068359', '108.626137', '151.872864', '110.759827',\n",
       "       '153.904266', '129.158463', '117.460449', '139.826904',\n",
       "       '118.264595', '189.686218', '144.428955', '113.172836',\n",
       "       '169.849365', '117.218124', '163.296371', '112.664276',\n",
       "       '163.799072', '117.021988', '134.311584', '107.62912', '191.336014',\n",
       "       '123.742744', '216.454376', '203.835754', '189.208832',\n",
       "       '218.992126', '199.848831', '186.609573', '188.478378',\n",
       "       '195.167877', '185.125565', '202.564484', '162.855881', '101.95594',\n",
       "       '170.239792', '109.683502', '116.62059', '155.84169', '103.577789',\n",
       "       '163.990295', '117.183617', '118.334038', '184.727631',\n",
       "       '112.344131', '121.37365', '165.99559', '116.044235', '171.441086',\n",
       "       '136.957611', '118.859093', '110.819038', '144.151352',\n",
       "       '116.735992', '199.085464', '141.827927', '116.657684',\n",
       "       '126.873253', '126.957237', '118.648232', '169.064697',\n",
       "       '126.535431', '115.877396', '161.396362', '112.962997',\n",
       "       '194.867279', '154.327332', '207.800781', '189.959534',\n",
       "       '186.833542', '217.142303', '191.703629', '204.051971',\n",
       "       '192.736649', '209.084045', '172.611374', '207.070267',\n",
       "       '193.231018', '208.691696', '185.423248', '215.640961',\n",
       "       '179.438522', '199.650574', '205.45546', '204.776855', '186.80864',\n",
       "       '216.210159', '119.720573', '219.103378', '192.049194',\n",
       "       '204.353531', '185.739838', '212.437851', '185.090775',\n",
       "       '218.200592', '186.933975', '202.769394', '188.05455', '191.444992',\n",
       "       '225.312851', '211.129822', '195.907761', '194.903717',\n",
       "       '180.975983', '209.354156', '187.460114', '217.309219',\n",
       "       '187.907578', '204.842529', '183.838882', '213.378265',\n",
       "       '183.983871', '205.39119', '193.335068', '206.418213', '190.924911',\n",
       "       '203.765976', '194.813217', '202.940857', '193.060806',\n",
       "       '206.552139', '190.720032', '179.69516', '210.878937', '197.936646',\n",
       "       '201.95163', '182.270599', '215.302094', '189.04953', '213.468872',\n",
       "       '199.043701', '208.882141', '214.049423', '193.967896',\n",
       "       '191.585892', '210.446381', '189.950439', '209.225128',\n",
       "       '218.567978', '185.481949', '216.248077', '185.429382', '211.52951',\n",
       "       '212.561172', '189.166275', '213.546997', '192.03038', '218.773636',\n",
       "       '207.673431', '192.271149', '197.164108', '201.314545',\n",
       "       '185.108154', '190.872406', '205.153183', '192.044922',\n",
       "       '208.405441', '192.639496', '198.514801', '200.639267',\n",
       "       '208.945541', '202.255936', '207.762985', '185.445404',\n",
       "       '215.419373', '189.210007', '177.307526', '207.103317',\n",
       "       '216.229248', '187.642792', '214.455002', '187.891754', '203.75264',\n",
       "       '200.177811', '204.354477', '195.834824', '192.497726',\n",
       "       '199.360489', '187.210678', '205.12645', '184.734985', '212.884628',\n",
       "       '186.060196', '218.382782', '197.471817', '194.645737',\n",
       "       '207.282593', '200.226105', '199.601013', '195.13382', '210.165176',\n",
       "       '184.212524', '218.668564', '213.034454', '185.153687',\n",
       "       '205.500427', '178.850128', '190.552414', '166.432617',\n",
       "       '206.847565', '201.800903', '195.850723', '192.579391',\n",
       "       '202.430771', '189.339401', '202.302917', '196.674728',\n",
       "       '208.983505', '192.526779', '200.875259', '218.601685',\n",
       "       '201.153488', '204.939713', '198.727188', '199.182175',\n",
       "       '192.959946', '217.605515', '219.522171', '126.137497',\n",
       "       '114.848938', '138.64476', '217.178009', '115.306526', '161.268082',\n",
       "       '127.012115', '124.957291', '127.434769', '198.33963', '203.597916',\n",
       "       '125.436012', '152.814499', '134.586563', '204.948715', '126.9422',\n",
       "       '116.837845', '140.473953', '121.165588', '164.279938',\n",
       "       '128.311295', '153.686325', '109.535324', '138.984268', '99.521202',\n",
       "       '204.884491', '144.185516', '103.178566', '137.370453', '122.21521',\n",
       "       '121.811043', '102.890213', '153.279556', '129.880142',\n",
       "       '152.512924', '122.816345', '204.164688', '153.297653',\n",
       "       '121.358711', '105.244186', '142.698364', '106.023338',\n",
       "       '163.849152', '111.080856', '140.890442', '124.087372',\n",
       "       '103.902199', '206.245163', '124.272812', '108.872124',\n",
       "       '169.670746', '105.727936', '155.530243', '117.149437',\n",
       "       '137.802948', '116.974983', '170.658936', '125.148064',\n",
       "       '203.812225', '158.210526', '129.158356', '171.192429',\n",
       "       '126.404488', '194.562592', '192.236755', '114.25383', '206.98349',\n",
       "       '212.62767', '209.539337', '132.52388', '112.055534', '132.644455',\n",
       "       '104.46096', '133.902328', '102.198837', '139.83989', '122.866837',\n",
       "       '98.161575', '162.380127', '105.950897', '140.674286', '110.018265',\n",
       "       '150.95639', '114.202698', '152.77298', '112.83091', '140.203812',\n",
       "       '150.273605', '106.716393', '149.930756', '113.974808',\n",
       "       '166.232132', '109.099297', '190.623764', '188.889023', '191.40834',\n",
       "       '189.228821', '187.675735', '101.762077', '187.706665', '187.17366',\n",
       "       '189.438889', '191.066742', '190.817856', '189.587479',\n",
       "       '188.512711', '190.345947', '190.128235', '191.215073', '214.96312',\n",
       "       '188.443268', '187.774521', '188.931702', '188.885025',\n",
       "       '189.537811', '131.925919', '109.726738', '166.228958',\n",
       "       '102.760986', '154.888229'], \n",
       "      dtype='<U32')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop, Adam, Adadelta\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "\n",
    "network = models.Sequential()\n",
    "nb_filters = 8\n",
    "nb_conv=3\n",
    "\n",
    "network.add(keras.layers.Conv2D(nb_filters, kernel_size=(nb_conv, nb_conv), activation='relu',data_format='channels_last',\n",
    "                        input_shape=(13, 301, 1), padding=\"same\" ) )\n",
    "\n",
    "#network.add(keras.layers.Conv2D(nb_filters, kernel_size=(nb_conv, nb_conv), activation='relu',data_format='channels_last',\n",
    " #                   padding=\"same\" ) )\n",
    "#network.add(keras.layers.Conv2D(nb_filters, kernel_size=(nb_conv, nb_conv), activation='relu',data_format='channels_last',\n",
    "#                        padding=\"same\" ) )\n",
    "network.add(keras.layers.Conv2D(nb_filters, kernel_size=(nb_conv, nb_conv), activation='relu',data_format='channels_last',\n",
    "                         padding=\"same\" ) )\n",
    "network.add(keras.layers.Conv2D(nb_filters, kernel_size=(nb_conv, nb_conv), activation='relu',data_format='channels_last',\n",
    "                        padding=\"same\" ) )\n",
    "\n",
    "network.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "#network.add(keras.layers.core.Dropout(0.25))\n",
    "\n",
    "\n",
    "#network.add(keras.layers.Conv2D(nb_filters*2, kernel_size=(nb_conv, nb_conv), activation='relu',data_format='channels_last',\n",
    "#                        padding=\"same\" ) )\n",
    "#network.add(keras.layers.Conv2D(nb_filters*2, kernel_size=(nb_conv, nb_conv), activation='relu',data_format='channels_last',\n",
    "#                        padding=\"same\" ) )\n",
    "network.add(keras.layers.Conv2D(nb_filters*2, kernel_size=(nb_conv, nb_conv), activation='relu',data_format='channels_last',\n",
    "                        padding=\"same\" ) )\n",
    "network.add(keras.layers.Conv2D(nb_filters*2, kernel_size=(nb_conv, nb_conv), activation='relu',data_format='channels_last',\n",
    "                        padding=\"same\" ) )\n",
    "\n",
    "\n",
    "#network.add(keras.layers.core.Dropout(0.5))\n",
    "\n",
    "network.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "network.add(Flatten())\n",
    "network.add(Dense(256, activation='relu'))\n",
    "\n",
    "#network.add(keras.layers.core.Dropout(0.5))\n",
    "network.add(Dense(128, activation='linear'))\n",
    "\n",
    "#network.add(keras.layers.core.Dropout(0.5))\n",
    "network.add(Dense(1, activation='linear'))\n",
    "\n",
    "\n",
    "network.compile(loss='mean_squared_error', optimizer=Adam(lr=0.0001))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 13, 301, 8)        80        \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 13, 301, 8)        584       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 13, 301, 8)        584       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 150, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 6, 150, 16)        1168      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 6, 150, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 3, 75, 16)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3600)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               921856    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 959,617\n",
      "Trainable params: 959,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(425, 13, 301, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " from sklearn.model_selection import train_test_split\n",
    "image_array = image_array.astype('float32') / 255\n",
    "X_train, X_test, y_train, y_test = train_test_split(image_array, image_labels, test_size=0.1, random_state=42)\n",
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['154.888229', '207.192993', '99.226677', '152.385468', '107.566246',\n",
       "       '160.122116', '117.07312', '142.775421', '106.087975', '136.91156',\n",
       "       '115.228539', '155.422791', '196.123276', '102.695328', '155.43663',\n",
       "       '103.915276', '134.6707', '109.622116', '122.580284', '110.657669',\n",
       "       '148.667923', '110.997406', '138.187805', '194.741257',\n",
       "       '116.708481', '153.688629', '109.461647', '159.791092', '99.025284',\n",
       "       '149.253143', '112.981071', '152.73642', '113.814461', '154.775711',\n",
       "       '154.024506', '119.933556', '150.810364', '106.79084', '151.905685',\n",
       "       '109.834557', '155.275299', '119.893448', '163.030563',\n",
       "       '110.753433', '162.431702', '113.085526', '107.304756',\n",
       "       '136.944763', '101.227585', '156.257278', '113.845451',\n",
       "       '149.071854', '112.559135', '161.975174', '119.5289', '116.918449',\n",
       "       '113.496269', '106.418221', '143.299026', '112.592712',\n",
       "       '159.796967', '122.819305', '142.271149', '126.278687',\n",
       "       '134.827942', '112.508675', '140.569519', '150.788742',\n",
       "       '103.932724', '152.390396', '127.402061', '156.969254',\n",
       "       '108.086983', '150.163681', '128.414688', '160.501678',\n",
       "       '114.126945', '152.103088', '126.168449', '113.485596', '160.20874',\n",
       "       '104.069572', '143.551559', '118.230461', '147.895111',\n",
       "       '123.896339', '148.661209', '115.170509', '160.108475',\n",
       "       '193.203476', '120.098717', '157.94548', '112.78447', '148.630539',\n",
       "       '117.561653', '110.304916', '142.263046', '111.001099',\n",
       "       '128.612152', '108.842636', '218.006729', '131.15126', '111.854416',\n",
       "       '117.055923', '131.180832', '114.533241', '153.970871',\n",
       "       '115.907433', '142.126953', '107.685333', '157.979538',\n",
       "       '141.478348', '219.901962', '134.224533', '132.074417',\n",
       "       '115.421509', '159.910156', '123.023697', '151.62236', '106.201797',\n",
       "       '148.096512', '117.821945', '137.095123', '200.170364',\n",
       "       '106.964043', '158.776459', '115.316208', '154.033157',\n",
       "       '111.735962', '149.585541', '122.285789', '151.512543',\n",
       "       '111.803528', '160.198532', '180.922379', '121.859787',\n",
       "       '149.068359', '108.626137', '151.872864', '110.759827',\n",
       "       '153.904266', '129.158463', '117.460449', '139.826904',\n",
       "       '118.264595', '189.686218', '144.428955', '113.172836',\n",
       "       '169.849365', '117.218124', '163.296371', '112.664276',\n",
       "       '163.799072', '117.021988', '134.311584', '107.62912', '191.336014',\n",
       "       '123.742744', '216.454376', '203.835754', '189.208832',\n",
       "       '218.992126', '199.848831', '186.609573', '188.478378',\n",
       "       '195.167877', '185.125565', '202.564484', '162.855881', '101.95594',\n",
       "       '170.239792', '109.683502', '116.62059', '155.84169', '103.577789',\n",
       "       '163.990295', '117.183617', '118.334038', '184.727631',\n",
       "       '112.344131', '121.37365', '165.99559', '116.044235', '171.441086',\n",
       "       '136.957611', '118.859093', '110.819038', '144.151352',\n",
       "       '116.735992', '199.085464', '141.827927', '116.657684',\n",
       "       '126.873253', '126.957237', '118.648232', '169.064697',\n",
       "       '126.535431', '115.877396', '161.396362', '112.962997',\n",
       "       '194.867279', '154.327332', '207.800781', '189.959534',\n",
       "       '186.833542', '217.142303', '191.703629', '204.051971',\n",
       "       '192.736649', '209.084045', '172.611374', '207.070267',\n",
       "       '193.231018', '208.691696', '185.423248', '215.640961',\n",
       "       '179.438522', '199.650574', '205.45546', '204.776855', '186.80864',\n",
       "       '216.210159', '119.720573', '219.103378', '192.049194',\n",
       "       '204.353531', '185.739838', '212.437851', '185.090775',\n",
       "       '218.200592', '186.933975', '202.769394', '188.05455', '191.444992',\n",
       "       '225.312851', '211.129822', '195.907761', '194.903717',\n",
       "       '180.975983', '209.354156', '187.460114', '217.309219',\n",
       "       '187.907578', '204.842529', '183.838882', '213.378265',\n",
       "       '183.983871', '205.39119', '193.335068', '206.418213', '190.924911',\n",
       "       '203.765976', '194.813217', '202.940857', '193.060806',\n",
       "       '206.552139', '190.720032', '179.69516', '210.878937', '197.936646',\n",
       "       '201.95163', '182.270599', '215.302094', '189.04953', '213.468872',\n",
       "       '199.043701', '208.882141', '214.049423', '193.967896',\n",
       "       '191.585892', '210.446381', '189.950439', '209.225128',\n",
       "       '218.567978', '185.481949', '216.248077', '185.429382', '211.52951',\n",
       "       '212.561172', '189.166275', '213.546997', '192.03038', '218.773636',\n",
       "       '207.673431', '192.271149', '197.164108', '201.314545',\n",
       "       '185.108154', '190.872406', '205.153183', '192.044922',\n",
       "       '208.405441', '192.639496', '198.514801', '200.639267',\n",
       "       '208.945541', '202.255936', '207.762985', '185.445404',\n",
       "       '215.419373', '189.210007', '177.307526', '207.103317',\n",
       "       '216.229248', '187.642792', '214.455002', '187.891754', '203.75264',\n",
       "       '200.177811', '204.354477', '195.834824', '192.497726',\n",
       "       '199.360489', '187.210678', '205.12645', '184.734985', '212.884628',\n",
       "       '186.060196', '218.382782', '197.471817', '194.645737',\n",
       "       '207.282593', '200.226105', '199.601013', '195.13382', '210.165176',\n",
       "       '184.212524', '218.668564', '213.034454', '185.153687',\n",
       "       '205.500427', '178.850128', '190.552414', '166.432617',\n",
       "       '206.847565', '201.800903', '195.850723', '192.579391',\n",
       "       '202.430771', '189.339401', '202.302917', '196.674728',\n",
       "       '208.983505', '192.526779', '200.875259', '218.601685',\n",
       "       '201.153488', '204.939713', '198.727188', '199.182175',\n",
       "       '192.959946', '217.605515', '219.522171', '126.137497',\n",
       "       '114.848938', '138.64476', '217.178009', '115.306526', '161.268082',\n",
       "       '127.012115', '124.957291', '127.434769', '198.33963', '203.597916',\n",
       "       '125.436012', '152.814499', '134.586563', '204.948715', '126.9422',\n",
       "       '116.837845', '140.473953', '121.165588', '164.279938',\n",
       "       '128.311295', '153.686325', '109.535324', '138.984268', '99.521202',\n",
       "       '204.884491', '144.185516', '103.178566', '137.370453', '122.21521',\n",
       "       '121.811043', '102.890213', '153.279556', '129.880142',\n",
       "       '152.512924', '122.816345', '204.164688', '153.297653',\n",
       "       '121.358711', '105.244186', '142.698364', '106.023338',\n",
       "       '163.849152', '111.080856', '140.890442', '124.087372',\n",
       "       '103.902199', '206.245163', '124.272812', '108.872124',\n",
       "       '169.670746', '105.727936', '155.530243', '117.149437',\n",
       "       '137.802948', '116.974983', '170.658936', '125.148064',\n",
       "       '203.812225', '158.210526', '129.158356', '171.192429',\n",
       "       '126.404488', '194.562592', '192.236755', '114.25383', '206.98349',\n",
       "       '212.62767', '209.539337', '132.52388', '112.055534', '132.644455',\n",
       "       '104.46096', '133.902328', '102.198837', '139.83989', '122.866837',\n",
       "       '98.161575', '162.380127', '105.950897', '140.674286', '110.018265',\n",
       "       '150.95639', '114.202698', '152.77298', '112.83091', '140.203812',\n",
       "       '150.273605', '106.716393', '149.930756', '113.974808',\n",
       "       '166.232132', '109.099297', '190.623764', '188.889023', '191.40834',\n",
       "       '189.228821', '187.675735', '101.762077', '187.706665', '187.17366',\n",
       "       '189.438889', '191.066742', '190.817856', '189.587479',\n",
       "       '188.512711', '190.345947', '190.128235', '191.215073', '214.96312',\n",
       "       '188.443268', '187.774521', '188.931702', '188.885025',\n",
       "       '189.537811', '131.925919', '109.726738', '166.228958',\n",
       "       '102.760986', '154.888229'], \n",
       "      dtype='<U32')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 382 samples, validate on 43 samples\n",
      "Epoch 1/1000\n",
      "382/382 [==============================] - 4s 9ms/step - loss: 27730.0615 - val_loss: 24461.5957\n",
      "Epoch 2/1000\n",
      "382/382 [==============================] - 0s 471us/step - loss: 27599.6269 - val_loss: 24328.2031\n",
      "Epoch 3/1000\n",
      "382/382 [==============================] - 0s 456us/step - loss: 27442.7823 - val_loss: 24167.0879\n",
      "Epoch 4/1000\n",
      "382/382 [==============================] - 0s 442us/step - loss: 27254.0735 - val_loss: 23966.0898\n",
      "Epoch 5/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 27010.2246 - val_loss: 23713.6230\n",
      "Epoch 6/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 26705.5910 - val_loss: 23389.3867\n",
      "Epoch 7/1000\n",
      "382/382 [==============================] - 0s 437us/step - loss: 26316.6384 - val_loss: 22971.4902\n",
      "Epoch 8/1000\n",
      "382/382 [==============================] - 0s 435us/step - loss: 25808.7712 - val_loss: 22435.3555\n",
      "Epoch 9/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 25159.1379 - val_loss: 21750.2344\n",
      "Epoch 10/1000\n",
      "382/382 [==============================] - 0s 437us/step - loss: 24330.4108 - val_loss: 20875.5723\n",
      "Epoch 11/1000\n",
      "382/382 [==============================] - 0s 435us/step - loss: 23287.7562 - val_loss: 19761.3516\n",
      "Epoch 12/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 21952.9279 - val_loss: 18368.6406\n",
      "Epoch 13/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 20289.7713 - val_loss: 16647.5547\n",
      "Epoch 14/1000\n",
      "382/382 [==============================] - 0s 448us/step - loss: 18240.1051 - val_loss: 14528.0557\n",
      "Epoch 15/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 15743.6949 - val_loss: 12011.6611\n",
      "Epoch 16/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 12780.2266 - val_loss: 9125.9883\n",
      "Epoch 17/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 9447.5669 - val_loss: 6073.4233\n",
      "Epoch 18/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 6105.3453 - val_loss: 3181.3706\n",
      "Epoch 19/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 2949.8045 - val_loss: 1040.9484\n",
      "Epoch 20/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 898.2439 - val_loss: 253.5596\n",
      "Epoch 21/1000\n",
      "382/382 [==============================] - 0s 435us/step - loss: 335.9534 - val_loss: 762.6469\n",
      "Epoch 22/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 813.5246 - val_loss: 1185.1833\n",
      "Epoch 23/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 930.1446 - val_loss: 849.3130\n",
      "Epoch 24/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 559.7107 - val_loss: 397.2921\n",
      "Epoch 25/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 280.8497 - val_loss: 234.3168\n",
      "Epoch 26/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 278.9221 - val_loss: 236.1280\n",
      "Epoch 27/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 326.7520 - val_loss: 242.7940\n",
      "Epoch 28/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 319.9924 - val_loss: 221.2892\n",
      "Epoch 29/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 273.8888 - val_loss: 209.5809\n",
      "Epoch 30/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 232.9434 - val_loss: 223.8801\n",
      "Epoch 31/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 226.3180 - val_loss: 243.9789\n",
      "Epoch 32/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 230.2231 - val_loss: 248.2560\n",
      "Epoch 33/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 224.2071 - val_loss: 228.8782\n",
      "Epoch 34/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 212.8829 - val_loss: 205.5770\n",
      "Epoch 35/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 203.8679 - val_loss: 189.8353\n",
      "Epoch 36/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 199.0319 - val_loss: 181.0346\n",
      "Epoch 37/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 194.5355 - val_loss: 176.2898\n",
      "Epoch 38/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 188.8443 - val_loss: 173.4344\n",
      "Epoch 39/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 182.7922 - val_loss: 171.0774\n",
      "Epoch 40/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 177.3019 - val_loss: 168.3976\n",
      "Epoch 41/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 172.1702 - val_loss: 164.8513\n",
      "Epoch 42/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 167.0635 - val_loss: 159.6691\n",
      "Epoch 43/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 161.6947 - val_loss: 152.7263\n",
      "Epoch 44/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 156.5197 - val_loss: 145.3566\n",
      "Epoch 45/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 151.4769 - val_loss: 138.7800\n",
      "Epoch 46/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 146.1872 - val_loss: 134.0511\n",
      "Epoch 47/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 140.9643 - val_loss: 129.3340\n",
      "Epoch 48/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 135.7572 - val_loss: 125.0103\n",
      "Epoch 49/1000\n",
      "382/382 [==============================] - 0s 435us/step - loss: 130.8702 - val_loss: 120.8114\n",
      "Epoch 50/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 125.8271 - val_loss: 115.0063\n",
      "Epoch 51/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 121.0681 - val_loss: 109.5656\n",
      "Epoch 52/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 116.1727 - val_loss: 105.1868\n",
      "Epoch 53/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 111.5467 - val_loss: 100.5627\n",
      "Epoch 54/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 106.9523 - val_loss: 94.7962\n",
      "Epoch 55/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 102.2790 - val_loss: 90.6491\n",
      "Epoch 56/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 97.8139 - val_loss: 85.5705\n",
      "Epoch 57/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 93.4550 - val_loss: 81.4035\n",
      "Epoch 58/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 89.1207 - val_loss: 78.0895\n",
      "Epoch 59/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 84.8933 - val_loss: 73.4908\n",
      "Epoch 60/1000\n",
      "382/382 [==============================] - 0s 432us/step - loss: 80.8234 - val_loss: 69.4647\n",
      "Epoch 61/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 76.8628 - val_loss: 65.3544\n",
      "Epoch 62/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 72.8970 - val_loss: 61.3672\n",
      "Epoch 63/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 69.1993 - val_loss: 57.4435\n",
      "Epoch 64/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 65.5582 - val_loss: 53.9966\n",
      "Epoch 65/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 62.0122 - val_loss: 50.2156\n",
      "Epoch 66/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 58.6979 - val_loss: 46.6187\n",
      "Epoch 67/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 55.3828 - val_loss: 44.1283\n",
      "Epoch 68/1000\n",
      "382/382 [==============================] - 0s 432us/step - loss: 52.2218 - val_loss: 41.0818\n",
      "Epoch 69/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 49.1477 - val_loss: 38.1866\n",
      "Epoch 70/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 46.2771 - val_loss: 34.9568\n",
      "Epoch 71/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 43.6872 - val_loss: 31.8020\n",
      "Epoch 72/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 40.9555 - val_loss: 29.7467\n",
      "Epoch 73/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 38.6214 - val_loss: 27.9042\n",
      "Epoch 74/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 36.3590 - val_loss: 25.4920\n",
      "Epoch 75/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382/382 [==============================] - 0s 421us/step - loss: 34.3241 - val_loss: 24.0740\n",
      "Epoch 76/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 32.2903 - val_loss: 21.9218\n",
      "Epoch 77/1000\n",
      "382/382 [==============================] - 0s 432us/step - loss: 30.5651 - val_loss: 20.0639\n",
      "Epoch 78/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 28.9753 - val_loss: 18.2789\n",
      "Epoch 79/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 27.4062 - val_loss: 17.1509\n",
      "Epoch 80/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 26.0565 - val_loss: 16.4410\n",
      "Epoch 81/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 24.8326 - val_loss: 15.2205\n",
      "Epoch 82/1000\n",
      "382/382 [==============================] - 0s 435us/step - loss: 23.7569 - val_loss: 14.2532\n",
      "Epoch 83/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 22.7637 - val_loss: 13.4074\n",
      "Epoch 84/1000\n",
      "382/382 [==============================] - 0s 437us/step - loss: 21.8691 - val_loss: 12.3823\n",
      "Epoch 85/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 21.1279 - val_loss: 11.5973\n",
      "Epoch 86/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 20.4111 - val_loss: 11.1644\n",
      "Epoch 87/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 19.8525 - val_loss: 10.7896\n",
      "Epoch 88/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 19.3365 - val_loss: 10.5257\n",
      "Epoch 89/1000\n",
      "382/382 [==============================] - 0s 432us/step - loss: 18.9158 - val_loss: 10.1415\n",
      "Epoch 90/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 18.5361 - val_loss: 9.8094\n",
      "Epoch 91/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 18.2180 - val_loss: 9.5889\n",
      "Epoch 92/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 17.9337 - val_loss: 9.4225\n",
      "Epoch 93/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 17.6742 - val_loss: 9.1936\n",
      "Epoch 94/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 17.5148 - val_loss: 9.1368\n",
      "Epoch 95/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 17.3156 - val_loss: 8.8902\n",
      "Epoch 96/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 17.1800 - val_loss: 8.7362\n",
      "Epoch 97/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 17.0069 - val_loss: 8.8289\n",
      "Epoch 98/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 16.9363 - val_loss: 9.0767\n",
      "Epoch 99/1000\n",
      "382/382 [==============================] - 0s 440us/step - loss: 16.8334 - val_loss: 8.9105\n",
      "Epoch 100/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 16.6976 - val_loss: 8.6046\n",
      "Epoch 101/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 16.6188 - val_loss: 8.3843\n",
      "Epoch 102/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 16.5835 - val_loss: 8.4584\n",
      "Epoch 103/1000\n",
      "382/382 [==============================] - 0s 435us/step - loss: 16.4921 - val_loss: 8.4935\n",
      "Epoch 104/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 16.4418 - val_loss: 8.7407\n",
      "Epoch 105/1000\n",
      "382/382 [==============================] - 0s 430us/step - loss: 16.4358 - val_loss: 8.5022\n",
      "Epoch 106/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 16.3039 - val_loss: 8.5353\n",
      "Epoch 107/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 16.2890 - val_loss: 8.5713\n",
      "Epoch 108/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 16.2105 - val_loss: 8.3386\n",
      "Epoch 109/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 16.1763 - val_loss: 8.2354\n",
      "Epoch 110/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 16.1304 - val_loss: 8.2527\n",
      "Epoch 111/1000\n",
      "382/382 [==============================] - 0s 432us/step - loss: 16.0772 - val_loss: 8.4756\n",
      "Epoch 112/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 16.0541 - val_loss: 8.3416\n",
      "Epoch 113/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 16.0163 - val_loss: 8.4129\n",
      "Epoch 114/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 15.9631 - val_loss: 8.1609\n",
      "Epoch 115/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 15.9833 - val_loss: 8.0157\n",
      "Epoch 116/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 15.8967 - val_loss: 8.3015\n",
      "Epoch 117/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 15.8955 - val_loss: 8.4388\n",
      "Epoch 118/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 15.8075 - val_loss: 8.1454\n",
      "Epoch 119/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 15.7669 - val_loss: 7.9377\n",
      "Epoch 120/1000\n",
      "382/382 [==============================] - 0s 435us/step - loss: 15.7385 - val_loss: 7.9449\n",
      "Epoch 121/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 15.7155 - val_loss: 7.9611\n",
      "Epoch 122/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 15.7772 - val_loss: 8.3170\n",
      "Epoch 123/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 15.6591 - val_loss: 7.9886\n",
      "Epoch 124/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 15.5883 - val_loss: 7.8860\n",
      "Epoch 125/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 15.5726 - val_loss: 7.9005\n",
      "Epoch 126/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 15.5297 - val_loss: 7.9453\n",
      "Epoch 127/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 15.4821 - val_loss: 7.8716\n",
      "Epoch 128/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 15.4465 - val_loss: 7.8805\n",
      "Epoch 129/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 15.4161 - val_loss: 7.8635\n",
      "Epoch 130/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 15.3888 - val_loss: 7.8366\n",
      "Epoch 131/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 15.3600 - val_loss: 7.6730\n",
      "Epoch 132/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 15.3263 - val_loss: 7.7141\n",
      "Epoch 133/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 15.2886 - val_loss: 7.7380\n",
      "Epoch 134/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 15.3099 - val_loss: 7.9232\n",
      "Epoch 135/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 15.2811 - val_loss: 7.6024\n",
      "Epoch 136/1000\n",
      "382/382 [==============================] - 0s 440us/step - loss: 15.2102 - val_loss: 7.5439\n",
      "Epoch 137/1000\n",
      "382/382 [==============================] - 0s 435us/step - loss: 15.1650 - val_loss: 7.6950\n",
      "Epoch 138/1000\n",
      "382/382 [==============================] - 0s 453us/step - loss: 15.2392 - val_loss: 7.8472\n",
      "Epoch 139/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 15.2016 - val_loss: 7.4452\n",
      "Epoch 140/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 15.0906 - val_loss: 7.5116\n",
      "Epoch 141/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 15.0414 - val_loss: 7.6015\n",
      "Epoch 142/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 15.0160 - val_loss: 7.5876\n",
      "Epoch 143/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 14.9980 - val_loss: 7.5202\n",
      "Epoch 144/1000\n",
      "382/382 [==============================] - 0s 432us/step - loss: 14.9789 - val_loss: 7.4722\n",
      "Epoch 145/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 14.9406 - val_loss: 7.4927\n",
      "Epoch 146/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 14.9343 - val_loss: 7.3788\n",
      "Epoch 147/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 14.9275 - val_loss: 7.5111\n",
      "Epoch 148/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 14.8649 - val_loss: 7.3601\n",
      "Epoch 149/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 14.8245 - val_loss: 7.3803\n",
      "Epoch 150/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 14.8217 - val_loss: 7.3270\n",
      "Epoch 151/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 14.7751 - val_loss: 7.4010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 152/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 14.7485 - val_loss: 7.4207\n",
      "Epoch 153/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 14.7636 - val_loss: 7.3703\n",
      "Epoch 154/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 14.7228 - val_loss: 7.2630\n",
      "Epoch 155/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 14.6837 - val_loss: 7.1609\n",
      "Epoch 156/1000\n",
      "382/382 [==============================] - 0s 445us/step - loss: 14.6893 - val_loss: 7.1193\n",
      "Epoch 157/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 14.6357 - val_loss: 7.2422\n",
      "Epoch 158/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 14.6292 - val_loss: 7.1991\n",
      "Epoch 159/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 14.5840 - val_loss: 7.2642\n",
      "Epoch 160/1000\n",
      "382/382 [==============================] - 0s 435us/step - loss: 14.5657 - val_loss: 7.3309\n",
      "Epoch 161/1000\n",
      "382/382 [==============================] - 0s 435us/step - loss: 14.5560 - val_loss: 7.1491\n",
      "Epoch 162/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 14.5217 - val_loss: 7.1271\n",
      "Epoch 163/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 14.5565 - val_loss: 7.2323\n",
      "Epoch 164/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 14.4594 - val_loss: 7.1087\n",
      "Epoch 165/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 14.4429 - val_loss: 6.9907\n",
      "Epoch 166/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 14.4189 - val_loss: 7.0591\n",
      "Epoch 167/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 14.4824 - val_loss: 7.2740\n",
      "Epoch 168/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 14.3940 - val_loss: 7.0019\n",
      "Epoch 169/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 14.3450 - val_loss: 6.9614\n",
      "Epoch 170/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 14.3256 - val_loss: 6.9683\n",
      "Epoch 171/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 14.3091 - val_loss: 7.0005\n",
      "Epoch 172/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 14.3044 - val_loss: 6.9918\n",
      "Epoch 173/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 14.3012 - val_loss: 7.1032\n",
      "Epoch 174/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 14.2446 - val_loss: 6.9647\n",
      "Epoch 175/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 14.3268 - val_loss: 6.8028\n",
      "Epoch 176/1000\n",
      "382/382 [==============================] - 0s 435us/step - loss: 14.2161 - val_loss: 7.0289\n",
      "Epoch 177/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 14.1946 - val_loss: 7.0452\n",
      "Epoch 178/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 14.1948 - val_loss: 6.8953\n",
      "Epoch 179/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 14.1450 - val_loss: 6.7986\n",
      "Epoch 180/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 14.1794 - val_loss: 6.7314\n",
      "Epoch 181/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 14.1352 - val_loss: 7.0355\n",
      "Epoch 182/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 14.1104 - val_loss: 6.9231\n",
      "Epoch 183/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 14.0761 - val_loss: 6.8257\n",
      "Epoch 184/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 14.1786 - val_loss: 6.6804\n",
      "Epoch 185/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 14.0415 - val_loss: 7.0708\n",
      "Epoch 186/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 14.0931 - val_loss: 6.9441\n",
      "Epoch 187/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 14.0568 - val_loss: 6.6778\n",
      "Epoch 188/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 13.9916 - val_loss: 6.7237\n",
      "Epoch 189/1000\n",
      "382/382 [==============================] - 0s 440us/step - loss: 14.0347 - val_loss: 6.9876\n",
      "Epoch 190/1000\n",
      "382/382 [==============================] - 0s 432us/step - loss: 13.9872 - val_loss: 6.6696\n",
      "Epoch 191/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 13.9350 - val_loss: 6.7157\n",
      "Epoch 192/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 13.9043 - val_loss: 6.7118\n",
      "Epoch 193/1000\n",
      "382/382 [==============================] - 0s 435us/step - loss: 13.8830 - val_loss: 6.8149\n",
      "Epoch 194/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 13.8985 - val_loss: 6.7408\n",
      "Epoch 195/1000\n",
      "382/382 [==============================] - 0s 435us/step - loss: 13.8497 - val_loss: 6.6776\n",
      "Epoch 196/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 13.8401 - val_loss: 6.6884\n",
      "Epoch 197/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 13.8886 - val_loss: 6.5775\n",
      "Epoch 198/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 13.8467 - val_loss: 6.8735\n",
      "Epoch 199/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 13.8558 - val_loss: 6.5921\n",
      "Epoch 200/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 13.8660 - val_loss: 6.7248\n",
      "Epoch 201/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 13.7735 - val_loss: 6.5259\n",
      "Epoch 202/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 13.7389 - val_loss: 6.5794\n",
      "Epoch 203/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 13.7316 - val_loss: 6.7205\n",
      "Epoch 204/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 13.6990 - val_loss: 6.5836\n",
      "Epoch 205/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 13.7000 - val_loss: 6.4998\n",
      "Epoch 206/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 13.6585 - val_loss: 6.6334\n",
      "Epoch 207/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 13.6570 - val_loss: 6.6114\n",
      "Epoch 208/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 13.6365 - val_loss: 6.6588\n",
      "Epoch 209/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 13.6485 - val_loss: 6.4989\n",
      "Epoch 210/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 13.6458 - val_loss: 6.6267\n",
      "Epoch 211/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 13.5905 - val_loss: 6.4933\n",
      "Epoch 212/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 13.5740 - val_loss: 6.5087\n",
      "Epoch 213/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 13.5611 - val_loss: 6.4563\n",
      "Epoch 214/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 13.5411 - val_loss: 6.5190\n",
      "Epoch 215/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 13.5237 - val_loss: 6.4909\n",
      "Epoch 216/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 13.5181 - val_loss: 6.5071\n",
      "Epoch 217/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 13.5049 - val_loss: 6.5179\n",
      "Epoch 218/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 13.4904 - val_loss: 6.4433\n",
      "Epoch 219/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 13.5032 - val_loss: 6.4382\n",
      "Epoch 220/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 13.5522 - val_loss: 6.6785\n",
      "Epoch 221/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 13.4356 - val_loss: 6.3572\n",
      "Epoch 222/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 13.4397 - val_loss: 6.2945\n",
      "Epoch 223/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 13.4868 - val_loss: 6.3335\n",
      "Epoch 224/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 13.4493 - val_loss: 6.8352\n",
      "Epoch 225/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 13.4860 - val_loss: 6.3820\n",
      "Epoch 226/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 13.4113 - val_loss: 6.2994\n",
      "Epoch 227/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 13.3826 - val_loss: 6.4070\n",
      "Epoch 228/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382/382 [==============================] - 0s 429us/step - loss: 13.4640 - val_loss: 6.2731\n",
      "Epoch 229/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 13.3678 - val_loss: 6.7162\n",
      "Epoch 230/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 13.3822 - val_loss: 6.3276\n",
      "Epoch 231/1000\n",
      "382/382 [==============================] - 0s 414us/step - loss: 13.3119 - val_loss: 6.2427\n",
      "Epoch 232/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 13.3257 - val_loss: 6.3328\n",
      "Epoch 233/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 13.3216 - val_loss: 6.2394\n",
      "Epoch 234/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 13.2800 - val_loss: 6.4356\n",
      "Epoch 235/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 13.2571 - val_loss: 6.3411\n",
      "Epoch 236/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 13.2195 - val_loss: 6.2491\n",
      "Epoch 237/1000\n",
      "382/382 [==============================] - 0s 414us/step - loss: 13.3387 - val_loss: 6.2191\n",
      "Epoch 238/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 13.2729 - val_loss: 6.6203\n",
      "Epoch 239/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 13.2795 - val_loss: 6.2359\n",
      "Epoch 240/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 13.5880 - val_loss: 6.0626\n",
      "Epoch 241/1000\n",
      "382/382 [==============================] - 0s 414us/step - loss: 13.3422 - val_loss: 6.7924\n",
      "Epoch 242/1000\n",
      "382/382 [==============================] - 0s 414us/step - loss: 13.2916 - val_loss: 6.3992\n",
      "Epoch 243/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 13.1804 - val_loss: 6.0916\n",
      "Epoch 244/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 13.2413 - val_loss: 6.2440\n",
      "Epoch 245/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 13.1486 - val_loss: 6.3028\n",
      "Epoch 246/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 13.1073 - val_loss: 6.3028\n",
      "Epoch 247/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 13.1248 - val_loss: 6.1788\n",
      "Epoch 248/1000\n",
      "382/382 [==============================] - 0s 414us/step - loss: 13.0777 - val_loss: 6.3181\n",
      "Epoch 249/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 13.0915 - val_loss: 6.2348\n",
      "Epoch 250/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 13.0901 - val_loss: 6.0655\n",
      "Epoch 251/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 13.0596 - val_loss: 6.2882\n",
      "Epoch 252/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 13.0504 - val_loss: 6.2674\n",
      "Epoch 253/1000\n",
      "382/382 [==============================] - 0s 414us/step - loss: 13.0484 - val_loss: 6.0654\n",
      "Epoch 254/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 13.0173 - val_loss: 6.1959\n",
      "Epoch 255/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 13.0455 - val_loss: 6.2558\n",
      "Epoch 256/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 13.0726 - val_loss: 6.0293\n",
      "Epoch 257/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 13.1972 - val_loss: 6.4411\n",
      "Epoch 258/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 13.0908 - val_loss: 5.9996\n",
      "Epoch 259/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 12.9643 - val_loss: 6.1191\n",
      "Epoch 260/1000\n",
      "382/382 [==============================] - 0s 414us/step - loss: 13.1217 - val_loss: 6.4132\n",
      "Epoch 261/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 12.8878 - val_loss: 5.9693\n",
      "Epoch 262/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 13.1506 - val_loss: 5.9570\n",
      "Epoch 263/1000\n",
      "382/382 [==============================] - 0s 414us/step - loss: 12.8617 - val_loss: 6.6332\n",
      "Epoch 264/1000\n",
      "382/382 [==============================] - 0s 414us/step - loss: 13.0769 - val_loss: 6.1947\n",
      "Epoch 265/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 13.0148 - val_loss: 5.9004\n",
      "Epoch 266/1000\n",
      "382/382 [==============================] - 0s 414us/step - loss: 12.9853 - val_loss: 6.2562\n",
      "Epoch 267/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 12.8752 - val_loss: 6.1371\n",
      "Epoch 268/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 12.9056 - val_loss: 6.0106\n",
      "Epoch 269/1000\n",
      "382/382 [==============================] - 0s 414us/step - loss: 12.8303 - val_loss: 5.9612\n",
      "Epoch 270/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 12.8259 - val_loss: 6.0916\n",
      "Epoch 271/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 12.8024 - val_loss: 6.1217\n",
      "Epoch 272/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 12.8062 - val_loss: 6.0211\n",
      "Epoch 273/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 12.7804 - val_loss: 6.0869\n",
      "Epoch 274/1000\n",
      "382/382 [==============================] - 0s 414us/step - loss: 12.7784 - val_loss: 6.0629\n",
      "Epoch 275/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 12.8267 - val_loss: 6.0149\n",
      "Epoch 276/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 12.7296 - val_loss: 5.9539\n",
      "Epoch 277/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 12.7235 - val_loss: 6.0188\n",
      "Epoch 278/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 12.7256 - val_loss: 6.0802\n",
      "Epoch 279/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 12.6844 - val_loss: 5.9076\n",
      "Epoch 280/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 12.7500 - val_loss: 5.8832\n",
      "Epoch 281/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 12.6807 - val_loss: 6.2411\n",
      "Epoch 282/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 12.6953 - val_loss: 5.9540\n",
      "Epoch 283/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 12.7139 - val_loss: 5.8322\n",
      "Epoch 284/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 12.7042 - val_loss: 6.2300\n",
      "Epoch 285/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 12.6630 - val_loss: 5.9190\n",
      "Epoch 286/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 12.6440 - val_loss: 5.7900\n",
      "Epoch 287/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 12.6612 - val_loss: 6.0964\n",
      "Epoch 288/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 12.6354 - val_loss: 5.9299\n",
      "Epoch 289/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 12.6123 - val_loss: 5.9166\n",
      "Epoch 290/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 12.5642 - val_loss: 5.8167\n",
      "Epoch 291/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 12.6122 - val_loss: 5.9504\n",
      "Epoch 292/1000\n",
      "382/382 [==============================] - 0s 414us/step - loss: 12.5508 - val_loss: 5.8459\n",
      "Epoch 293/1000\n",
      "382/382 [==============================] - 0s 414us/step - loss: 12.5189 - val_loss: 5.9395\n",
      "Epoch 294/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 12.5291 - val_loss: 5.9010\n",
      "Epoch 295/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 12.4978 - val_loss: 5.8597\n",
      "Epoch 296/1000\n",
      "382/382 [==============================] - 0s 414us/step - loss: 12.5010 - val_loss: 5.8704\n",
      "Epoch 297/1000\n",
      "382/382 [==============================] - 0s 432us/step - loss: 12.5487 - val_loss: 5.8615\n",
      "Epoch 298/1000\n",
      "382/382 [==============================] - 0s 414us/step - loss: 12.4667 - val_loss: 5.7963\n",
      "Epoch 299/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 12.4648 - val_loss: 5.8278\n",
      "Epoch 300/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 12.6023 - val_loss: 5.8679\n",
      "Epoch 301/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 12.4692 - val_loss: 6.2574\n",
      "Epoch 302/1000\n",
      "382/382 [==============================] - 0s 414us/step - loss: 12.4876 - val_loss: 5.6779\n",
      "Epoch 303/1000\n",
      "382/382 [==============================] - 0s 414us/step - loss: 12.4488 - val_loss: 5.7800\n",
      "Epoch 304/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382/382 [==============================] - 0s 416us/step - loss: 12.5231 - val_loss: 6.0124\n",
      "Epoch 305/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 12.3906 - val_loss: 5.6851\n",
      "Epoch 306/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 12.4123 - val_loss: 5.8131\n",
      "Epoch 307/1000\n",
      "382/382 [==============================] - 0s 414us/step - loss: 12.4219 - val_loss: 5.9679\n",
      "Epoch 308/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 12.6400 - val_loss: 5.6380\n",
      "Epoch 309/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 12.4125 - val_loss: 6.2884\n",
      "Epoch 310/1000\n",
      "382/382 [==============================] - 0s 414us/step - loss: 12.4321 - val_loss: 5.6533\n",
      "Epoch 311/1000\n",
      "382/382 [==============================] - 0s 414us/step - loss: 12.3559 - val_loss: 5.7096\n",
      "Epoch 312/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 12.2883 - val_loss: 5.9880\n",
      "Epoch 313/1000\n",
      "382/382 [==============================] - 0s 414us/step - loss: 12.3248 - val_loss: 5.6756\n",
      "Epoch 314/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 12.6145 - val_loss: 5.6076\n",
      "Epoch 315/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 12.5650 - val_loss: 6.5753\n",
      "Epoch 316/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 12.3881 - val_loss: 5.5853\n",
      "Epoch 317/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 12.5321 - val_loss: 5.8430\n",
      "Epoch 318/1000\n",
      "382/382 [==============================] - 0s 432us/step - loss: 12.4139 - val_loss: 5.9332\n",
      "Epoch 319/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 12.2250 - val_loss: 5.5526\n",
      "Epoch 320/1000\n",
      "382/382 [==============================] - 0s 414us/step - loss: 12.3820 - val_loss: 5.8635\n",
      "Epoch 321/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 12.1803 - val_loss: 5.6393\n",
      "Epoch 322/1000\n",
      "382/382 [==============================] - 0s 435us/step - loss: 12.1693 - val_loss: 5.6521\n",
      "Epoch 323/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 12.2059 - val_loss: 5.6205\n",
      "Epoch 324/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 12.1293 - val_loss: 5.7193\n",
      "Epoch 325/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 12.1180 - val_loss: 5.6920\n",
      "Epoch 326/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 12.1119 - val_loss: 5.5985\n",
      "Epoch 327/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 12.0999 - val_loss: 5.6851\n",
      "Epoch 328/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 12.1640 - val_loss: 5.5613\n",
      "Epoch 329/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 12.0281 - val_loss: 5.8163\n",
      "Epoch 330/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 12.1047 - val_loss: 5.5690\n",
      "Epoch 331/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 12.0157 - val_loss: 5.6094\n",
      "Epoch 332/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 12.0577 - val_loss: 5.6365\n",
      "Epoch 333/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 12.1373 - val_loss: 5.5056\n",
      "Epoch 334/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 12.0914 - val_loss: 5.9028\n",
      "Epoch 335/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 12.0851 - val_loss: 5.4334\n",
      "Epoch 336/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 11.9730 - val_loss: 5.6959\n",
      "Epoch 337/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 11.9729 - val_loss: 5.5215\n",
      "Epoch 338/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 11.9513 - val_loss: 5.5114\n",
      "Epoch 339/1000\n",
      "382/382 [==============================] - 0s 414us/step - loss: 11.9138 - val_loss: 5.7019\n",
      "Epoch 340/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 12.3268 - val_loss: 5.6846\n",
      "Epoch 341/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 12.1148 - val_loss: 5.4851\n",
      "Epoch 342/1000\n",
      "382/382 [==============================] - 0s 414us/step - loss: 11.9169 - val_loss: 6.2551\n",
      "Epoch 343/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 12.1372 - val_loss: 5.4303\n",
      "Epoch 344/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 11.9253 - val_loss: 5.4157\n",
      "Epoch 345/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 11.9309 - val_loss: 5.7264\n",
      "Epoch 346/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 12.0532 - val_loss: 5.3834\n",
      "Epoch 347/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 11.9879 - val_loss: 5.7827\n",
      "Epoch 348/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 11.9727 - val_loss: 5.3451\n",
      "Epoch 349/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 11.7665 - val_loss: 5.8882\n",
      "Epoch 350/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 11.9031 - val_loss: 5.3697\n",
      "Epoch 351/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 11.8143 - val_loss: 5.3881\n",
      "Epoch 352/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 11.8379 - val_loss: 5.6755\n",
      "Epoch 353/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 11.9450 - val_loss: 5.2995\n",
      "Epoch 354/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 11.7045 - val_loss: 6.1229\n",
      "Epoch 355/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 12.0170 - val_loss: 5.2776\n",
      "Epoch 356/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 11.8688 - val_loss: 5.5413\n",
      "Epoch 357/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 11.8802 - val_loss: 5.4982\n",
      "Epoch 358/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 11.6912 - val_loss: 5.2637\n",
      "Epoch 359/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 11.6763 - val_loss: 5.7136\n",
      "Epoch 360/1000\n",
      "382/382 [==============================] - 0s 414us/step - loss: 11.7718 - val_loss: 5.3216\n",
      "Epoch 361/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 11.6821 - val_loss: 5.4416\n",
      "Epoch 362/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 11.6503 - val_loss: 5.3531\n",
      "Epoch 363/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 11.5369 - val_loss: 5.5400\n",
      "Epoch 364/1000\n",
      "382/382 [==============================] - 0s 414us/step - loss: 11.6125 - val_loss: 5.2634\n",
      "Epoch 365/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 11.5820 - val_loss: 5.3249\n",
      "Epoch 366/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 11.5183 - val_loss: 5.3012\n",
      "Epoch 367/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 11.5304 - val_loss: 5.3396\n",
      "Epoch 368/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 11.5558 - val_loss: 5.3471\n",
      "Epoch 369/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 11.4970 - val_loss: 5.1899\n",
      "Epoch 370/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 11.5390 - val_loss: 5.2665\n",
      "Epoch 371/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 11.7081 - val_loss: 5.4550\n",
      "Epoch 372/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 11.4386 - val_loss: 5.1482\n",
      "Epoch 373/1000\n",
      "382/382 [==============================] - 0s 432us/step - loss: 11.4743 - val_loss: 5.5515\n",
      "Epoch 374/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 11.5052 - val_loss: 5.1850\n",
      "Epoch 375/1000\n",
      "382/382 [==============================] - 0s 440us/step - loss: 11.3995 - val_loss: 5.2867\n",
      "Epoch 376/1000\n",
      "382/382 [==============================] - 0s 469us/step - loss: 11.3891 - val_loss: 5.2410\n",
      "Epoch 377/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 11.4210 - val_loss: 5.2709\n",
      "Epoch 378/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 11.3972 - val_loss: 5.1380\n",
      "Epoch 379/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 11.3380 - val_loss: 5.3788\n",
      "Epoch 380/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382/382 [==============================] - 0s 416us/step - loss: 11.4872 - val_loss: 5.1457\n",
      "Epoch 381/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 11.4848 - val_loss: 5.1597\n",
      "Epoch 382/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 11.2758 - val_loss: 5.5975\n",
      "Epoch 383/1000\n",
      "382/382 [==============================] - 0s 437us/step - loss: 11.5144 - val_loss: 5.0443\n",
      "Epoch 384/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 11.3004 - val_loss: 5.5356\n",
      "Epoch 385/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 11.2483 - val_loss: 5.0342\n",
      "Epoch 386/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 11.3868 - val_loss: 5.4476\n",
      "Epoch 387/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 11.3911 - val_loss: 5.0495\n",
      "Epoch 388/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 11.2023 - val_loss: 5.3143\n",
      "Epoch 389/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 11.5372 - val_loss: 5.0373\n",
      "Epoch 390/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 11.2451 - val_loss: 5.1158\n",
      "Epoch 391/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 11.1242 - val_loss: 5.3010\n",
      "Epoch 392/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 11.1226 - val_loss: 5.0257\n",
      "Epoch 393/1000\n",
      "382/382 [==============================] - 0s 414us/step - loss: 11.3382 - val_loss: 5.3465\n",
      "Epoch 394/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 11.2288 - val_loss: 4.9906\n",
      "Epoch 395/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 11.2569 - val_loss: 5.1203\n",
      "Epoch 396/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 11.2446 - val_loss: 5.1457\n",
      "Epoch 397/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 11.5830 - val_loss: 4.9987\n",
      "Epoch 398/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 11.6486 - val_loss: 5.4222\n",
      "Epoch 399/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 11.4282 - val_loss: 4.9724\n",
      "Epoch 400/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 11.4900 - val_loss: 5.6910\n",
      "Epoch 401/1000\n",
      "382/382 [==============================] - 0s 422us/step - loss: 10.9416 - val_loss: 5.0534\n",
      "Epoch 402/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 11.3886 - val_loss: 5.9495\n",
      "Epoch 403/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 11.5853 - val_loss: 4.9212\n",
      "Epoch 404/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 11.5934 - val_loss: 5.0698\n",
      "Epoch 405/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 11.7184 - val_loss: 5.1633\n",
      "Epoch 406/1000\n",
      "382/382 [==============================] - 0s 414us/step - loss: 11.0828 - val_loss: 4.8874\n",
      "Epoch 407/1000\n",
      "382/382 [==============================] - 0s 435us/step - loss: 11.1471 - val_loss: 5.4273\n",
      "Epoch 408/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 10.9982 - val_loss: 4.8572\n",
      "Epoch 409/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 10.8991 - val_loss: 5.2365\n",
      "Epoch 410/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 11.1787 - val_loss: 4.8629\n",
      "Epoch 411/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 11.5834 - val_loss: 5.0589\n",
      "Epoch 412/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 11.3389 - val_loss: 4.9554\n",
      "Epoch 413/1000\n",
      "382/382 [==============================] - 0s 414us/step - loss: 11.2524 - val_loss: 4.8462\n",
      "Epoch 414/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 10.9953 - val_loss: 5.6227\n",
      "Epoch 415/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 11.4771 - val_loss: 4.8586\n",
      "Epoch 416/1000\n",
      "382/382 [==============================] - 0s 437us/step - loss: 10.9970 - val_loss: 5.8797\n",
      "Epoch 417/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 10.8846 - val_loss: 4.8825\n",
      "Epoch 418/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 11.2845 - val_loss: 5.0839\n",
      "Epoch 419/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 10.7809 - val_loss: 4.8051\n",
      "Epoch 420/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 11.1001 - val_loss: 4.8576\n",
      "Epoch 421/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 11.1749 - val_loss: 4.7891\n",
      "Epoch 422/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 11.0224 - val_loss: 5.4019\n",
      "Epoch 423/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 11.1689 - val_loss: 4.7647\n",
      "Epoch 424/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 11.0858 - val_loss: 5.4676\n",
      "Epoch 425/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 10.9774 - val_loss: 4.7222\n",
      "Epoch 426/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 10.6064 - val_loss: 5.5759\n",
      "Epoch 427/1000\n",
      "382/382 [==============================] - 0s 435us/step - loss: 10.7666 - val_loss: 4.7212\n",
      "Epoch 428/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 10.7485 - val_loss: 5.1790\n",
      "Epoch 429/1000\n",
      "382/382 [==============================] - 0s 432us/step - loss: 10.9439 - val_loss: 4.7079\n",
      "Epoch 430/1000\n",
      "382/382 [==============================] - 0s 442us/step - loss: 10.6987 - val_loss: 4.7183\n",
      "Epoch 431/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 10.7100 - val_loss: 4.8625\n",
      "Epoch 432/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 10.5901 - val_loss: 4.7686\n",
      "Epoch 433/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 10.5755 - val_loss: 4.7228\n",
      "Epoch 434/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 10.7315 - val_loss: 4.9265\n",
      "Epoch 435/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 10.8481 - val_loss: 4.6729\n",
      "Epoch 436/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 10.5365 - val_loss: 4.7160\n",
      "Epoch 437/1000\n",
      "382/382 [==============================] - 0s 432us/step - loss: 10.5142 - val_loss: 4.8439\n",
      "Epoch 438/1000\n",
      "382/382 [==============================] - 0s 432us/step - loss: 10.4895 - val_loss: 4.6670\n",
      "Epoch 439/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 10.3983 - val_loss: 5.0531\n",
      "Epoch 440/1000\n",
      "382/382 [==============================] - 0s 432us/step - loss: 10.6049 - val_loss: 4.6024\n",
      "Epoch 441/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 10.5623 - val_loss: 4.9807\n",
      "Epoch 442/1000\n",
      "382/382 [==============================] - 0s 423us/step - loss: 10.7964 - val_loss: 4.5891\n",
      "Epoch 443/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 10.5215 - val_loss: 4.6563\n",
      "Epoch 444/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 10.5211 - val_loss: 4.7267\n",
      "Epoch 445/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 10.5732 - val_loss: 4.6226\n",
      "Epoch 446/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 10.4680 - val_loss: 4.8332\n",
      "Epoch 447/1000\n",
      "382/382 [==============================] - 0s 442us/step - loss: 10.3476 - val_loss: 4.5433\n",
      "Epoch 448/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 10.3823 - val_loss: 4.7760\n",
      "Epoch 449/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 10.1908 - val_loss: 4.5274\n",
      "Epoch 450/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 10.2476 - val_loss: 4.8371\n",
      "Epoch 451/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 10.2297 - val_loss: 4.7095\n",
      "Epoch 452/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 10.2842 - val_loss: 4.6285\n",
      "Epoch 453/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 10.1437 - val_loss: 4.9971\n",
      "Epoch 454/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 10.2185 - val_loss: 4.5530\n",
      "Epoch 455/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 10.9209 - val_loss: 5.2027\n",
      "Epoch 456/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382/382 [==============================] - 0s 427us/step - loss: 10.7149 - val_loss: 4.4700\n",
      "Epoch 457/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 10.3577 - val_loss: 4.6394\n",
      "Epoch 458/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 10.5716 - val_loss: 4.4445\n",
      "Epoch 459/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 10.6647 - val_loss: 5.2319\n",
      "Epoch 460/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 10.4923 - val_loss: 4.4211\n",
      "Epoch 461/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 11.2516 - val_loss: 4.8333\n",
      "Epoch 462/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 10.7764 - val_loss: 4.4759\n",
      "Epoch 463/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 10.0593 - val_loss: 4.4779\n",
      "Epoch 464/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 10.2186 - val_loss: 4.9756\n",
      "Epoch 465/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 11.0388 - val_loss: 4.4911\n",
      "Epoch 466/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 11.4735 - val_loss: 4.5494\n",
      "Epoch 467/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 10.2138 - val_loss: 5.0254\n",
      "Epoch 468/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 10.0798 - val_loss: 4.4187\n",
      "Epoch 469/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 10.0663 - val_loss: 4.7987\n",
      "Epoch 470/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 10.1188 - val_loss: 4.4346\n",
      "Epoch 471/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 10.0210 - val_loss: 4.4366\n",
      "Epoch 472/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 9.9855 - val_loss: 4.5341\n",
      "Epoch 473/1000\n",
      "382/382 [==============================] - 0s 432us/step - loss: 10.4164 - val_loss: 4.3266\n",
      "Epoch 474/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 10.2537 - val_loss: 4.3962\n",
      "Epoch 475/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 10.2538 - val_loss: 4.4791\n",
      "Epoch 476/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 9.8667 - val_loss: 4.3187\n",
      "Epoch 477/1000\n",
      "382/382 [==============================] - 0s 432us/step - loss: 9.7945 - val_loss: 4.6096\n",
      "Epoch 478/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 9.8524 - val_loss: 4.3189\n",
      "Epoch 479/1000\n",
      "382/382 [==============================] - 0s 437us/step - loss: 10.0253 - val_loss: 4.5961\n",
      "Epoch 480/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 9.9120 - val_loss: 4.3581\n",
      "Epoch 481/1000\n",
      "382/382 [==============================] - 0s 432us/step - loss: 9.7252 - val_loss: 4.3395\n",
      "Epoch 482/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 9.8267 - val_loss: 4.2942\n",
      "Epoch 483/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 9.6875 - val_loss: 4.3388\n",
      "Epoch 484/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 9.6251 - val_loss: 4.3533\n",
      "Epoch 485/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 9.7463 - val_loss: 4.4872\n",
      "Epoch 486/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 9.7631 - val_loss: 4.2192\n",
      "Epoch 487/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 9.6347 - val_loss: 4.3155\n",
      "Epoch 488/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 9.6417 - val_loss: 4.4032\n",
      "Epoch 489/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 9.6013 - val_loss: 4.1841\n",
      "Epoch 490/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 9.7214 - val_loss: 4.7488\n",
      "Epoch 491/1000\n",
      "382/382 [==============================] - 0s 432us/step - loss: 9.9920 - val_loss: 4.2096\n",
      "Epoch 492/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 9.9824 - val_loss: 5.1186\n",
      "Epoch 493/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 9.7497 - val_loss: 4.3692\n",
      "Epoch 494/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 10.0882 - val_loss: 5.9592\n",
      "Epoch 495/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 10.1183 - val_loss: 4.3406\n",
      "Epoch 496/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 9.7655 - val_loss: 4.9699\n",
      "Epoch 497/1000\n",
      "382/382 [==============================] - 0s 432us/step - loss: 10.0594 - val_loss: 4.2295\n",
      "Epoch 498/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 9.9709 - val_loss: 5.2019\n",
      "Epoch 499/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 10.1115 - val_loss: 4.1510\n",
      "Epoch 500/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 10.0678 - val_loss: 4.4817\n",
      "Epoch 501/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 9.4613 - val_loss: 4.1082\n",
      "Epoch 502/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 9.3881 - val_loss: 4.3890\n",
      "Epoch 503/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 9.5702 - val_loss: 4.4152\n",
      "Epoch 504/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 9.4922 - val_loss: 4.0538\n",
      "Epoch 505/1000\n",
      "382/382 [==============================] - 0s 432us/step - loss: 9.4618 - val_loss: 4.7968\n",
      "Epoch 506/1000\n",
      "382/382 [==============================] - 0s 432us/step - loss: 9.8242 - val_loss: 4.0628\n",
      "Epoch 507/1000\n",
      "382/382 [==============================] - 0s 432us/step - loss: 9.2881 - val_loss: 4.8168\n",
      "Epoch 508/1000\n",
      "382/382 [==============================] - 0s 448us/step - loss: 9.3987 - val_loss: 4.0240\n",
      "Epoch 509/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 9.3388 - val_loss: 4.1194\n",
      "Epoch 510/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 9.3164 - val_loss: 4.1215\n",
      "Epoch 511/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 9.4771 - val_loss: 3.9857\n",
      "Epoch 512/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 9.2166 - val_loss: 4.0860\n",
      "Epoch 513/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 9.1002 - val_loss: 4.0576\n",
      "Epoch 514/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 9.1421 - val_loss: 4.0304\n",
      "Epoch 515/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 9.0812 - val_loss: 4.1388\n",
      "Epoch 516/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 9.2093 - val_loss: 3.9457\n",
      "Epoch 517/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 9.4881 - val_loss: 5.0513\n",
      "Epoch 518/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 9.5936 - val_loss: 3.9302\n",
      "Epoch 519/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 9.2646 - val_loss: 4.7209\n",
      "Epoch 520/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 9.2356 - val_loss: 3.9075\n",
      "Epoch 521/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 9.1195 - val_loss: 4.3195\n",
      "Epoch 522/1000\n",
      "382/382 [==============================] - 0s 437us/step - loss: 9.4452 - val_loss: 4.1173\n",
      "Epoch 523/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 8.9610 - val_loss: 3.8784\n",
      "Epoch 524/1000\n",
      "382/382 [==============================] - 0s 445us/step - loss: 8.9265 - val_loss: 4.6332\n",
      "Epoch 525/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 9.1880 - val_loss: 3.9114\n",
      "Epoch 526/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 9.1217 - val_loss: 4.4874\n",
      "Epoch 527/1000\n",
      "382/382 [==============================] - 0s 432us/step - loss: 8.9535 - val_loss: 3.9210\n",
      "Epoch 528/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 9.3299 - val_loss: 4.1128\n",
      "Epoch 529/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 9.3072 - val_loss: 4.0938\n",
      "Epoch 530/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 9.1267 - val_loss: 3.8161\n",
      "Epoch 531/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 9.2261 - val_loss: 4.2425\n",
      "Epoch 532/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 9.0263 - val_loss: 3.9787\n",
      "Epoch 533/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382/382 [==============================] - 0s 424us/step - loss: 9.3050 - val_loss: 4.1402\n",
      "Epoch 534/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 8.7339 - val_loss: 3.7663\n",
      "Epoch 535/1000\n",
      "382/382 [==============================] - 0s 414us/step - loss: 8.8814 - val_loss: 4.2152\n",
      "Epoch 536/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 8.9054 - val_loss: 3.7593\n",
      "Epoch 537/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 9.0056 - val_loss: 3.7344\n",
      "Epoch 538/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 8.8947 - val_loss: 5.1286\n",
      "Epoch 539/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 9.7078 - val_loss: 4.0544\n",
      "Epoch 540/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 9.3167 - val_loss: 4.4083\n",
      "Epoch 541/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 8.9690 - val_loss: 3.9039\n",
      "Epoch 542/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 8.6580 - val_loss: 3.7171\n",
      "Epoch 543/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 8.9093 - val_loss: 4.7674\n",
      "Epoch 544/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 9.1214 - val_loss: 3.8392\n",
      "Epoch 545/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 8.8005 - val_loss: 4.4922\n",
      "Epoch 546/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 8.7868 - val_loss: 3.6657\n",
      "Epoch 547/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 8.7105 - val_loss: 3.8657\n",
      "Epoch 548/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 8.7016 - val_loss: 3.9351\n",
      "Epoch 549/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 8.6266 - val_loss: 3.6359\n",
      "Epoch 550/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 8.4802 - val_loss: 3.9309\n",
      "Epoch 551/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 8.4822 - val_loss: 3.6227\n",
      "Epoch 552/1000\n",
      "382/382 [==============================] - 0s 435us/step - loss: 8.4733 - val_loss: 3.6306\n",
      "Epoch 553/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 8.4945 - val_loss: 4.0133\n",
      "Epoch 554/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 8.3730 - val_loss: 3.8390\n",
      "Epoch 555/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 8.6819 - val_loss: 4.8006\n",
      "Epoch 556/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 8.6056 - val_loss: 3.8531\n",
      "Epoch 557/1000\n",
      "382/382 [==============================] - 0s 414us/step - loss: 8.9730 - val_loss: 3.7420\n",
      "Epoch 558/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 8.7446 - val_loss: 4.0976\n",
      "Epoch 559/1000\n",
      "382/382 [==============================] - 0s 442us/step - loss: 8.4881 - val_loss: 3.5318\n",
      "Epoch 560/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 8.2507 - val_loss: 4.2687\n",
      "Epoch 561/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 8.3477 - val_loss: 3.5138\n",
      "Epoch 562/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 8.3644 - val_loss: 3.5427\n",
      "Epoch 563/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 8.2141 - val_loss: 3.5174\n",
      "Epoch 564/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 8.3508 - val_loss: 3.4997\n",
      "Epoch 565/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 8.2454 - val_loss: 4.2998\n",
      "Epoch 566/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 8.6268 - val_loss: 4.0892\n",
      "Epoch 567/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 8.8989 - val_loss: 5.0988\n",
      "Epoch 568/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 8.8716 - val_loss: 3.4411\n",
      "Epoch 569/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 8.3269 - val_loss: 3.4897\n",
      "Epoch 570/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 8.5313 - val_loss: 4.5416\n",
      "Epoch 571/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 8.4340 - val_loss: 3.5486\n",
      "Epoch 572/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 8.1965 - val_loss: 4.3570\n",
      "Epoch 573/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 8.0371 - val_loss: 3.4784\n",
      "Epoch 574/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 8.3245 - val_loss: 3.6241\n",
      "Epoch 575/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 7.9803 - val_loss: 3.4533\n",
      "Epoch 576/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 8.1330 - val_loss: 3.3943\n",
      "Epoch 577/1000\n",
      "382/382 [==============================] - 0s 440us/step - loss: 8.3958 - val_loss: 5.3074\n",
      "Epoch 578/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 8.8874 - val_loss: 4.8816\n",
      "Epoch 579/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 10.1243 - val_loss: 6.4974\n",
      "Epoch 580/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 9.0521 - val_loss: 4.2282\n",
      "Epoch 581/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 9.7129 - val_loss: 4.8006\n",
      "Epoch 582/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 8.9740 - val_loss: 3.5509\n",
      "Epoch 583/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 8.8131 - val_loss: 4.3548\n",
      "Epoch 584/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 8.1297 - val_loss: 3.4012\n",
      "Epoch 585/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 8.0249 - val_loss: 3.8863\n",
      "Epoch 586/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 7.8645 - val_loss: 3.2589\n",
      "Epoch 587/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 7.8997 - val_loss: 3.4978\n",
      "Epoch 588/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 7.8037 - val_loss: 3.3371\n",
      "Epoch 589/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 8.1012 - val_loss: 3.3522\n",
      "Epoch 590/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 8.0630 - val_loss: 3.8928\n",
      "Epoch 591/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 7.8062 - val_loss: 3.2795\n",
      "Epoch 592/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 7.7412 - val_loss: 3.4163\n",
      "Epoch 593/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 7.7123 - val_loss: 3.2776\n",
      "Epoch 594/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 7.7481 - val_loss: 3.2513\n",
      "Epoch 595/1000\n",
      "382/382 [==============================] - 0s 442us/step - loss: 7.6704 - val_loss: 3.3691\n",
      "Epoch 596/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 7.6035 - val_loss: 3.2529\n",
      "Epoch 597/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 7.6652 - val_loss: 3.2132\n",
      "Epoch 598/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 7.7819 - val_loss: 4.1657\n",
      "Epoch 599/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 7.6589 - val_loss: 3.7393\n",
      "Epoch 600/1000\n",
      "382/382 [==============================] - 0s 440us/step - loss: 8.5261 - val_loss: 4.8191\n",
      "Epoch 601/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 8.2867 - val_loss: 3.2090\n",
      "Epoch 602/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 7.7679 - val_loss: 3.1531\n",
      "Epoch 603/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 7.5039 - val_loss: 3.6102\n",
      "Epoch 604/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 7.4416 - val_loss: 3.2858\n",
      "Epoch 605/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 7.7561 - val_loss: 3.4963\n",
      "Epoch 606/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 7.8938 - val_loss: 4.3262\n",
      "Epoch 607/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 8.0575 - val_loss: 3.4925\n",
      "Epoch 608/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 7.5366 - val_loss: 4.3174\n",
      "Epoch 609/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 7.6215 - val_loss: 3.0884\n",
      "Epoch 610/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382/382 [==============================] - 0s 419us/step - loss: 7.3440 - val_loss: 3.9674\n",
      "Epoch 611/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 8.0059 - val_loss: 3.1175\n",
      "Epoch 612/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 7.1602 - val_loss: 4.1851\n",
      "Epoch 613/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 7.4369 - val_loss: 3.0861\n",
      "Epoch 614/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 7.7291 - val_loss: 3.0460\n",
      "Epoch 615/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 7.8109 - val_loss: 4.2675\n",
      "Epoch 616/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 7.6034 - val_loss: 3.0332\n",
      "Epoch 617/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 7.1281 - val_loss: 3.5313\n",
      "Epoch 618/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 7.2768 - val_loss: 3.0038\n",
      "Epoch 619/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 7.2008 - val_loss: 3.0492\n",
      "Epoch 620/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 7.1064 - val_loss: 3.3794\n",
      "Epoch 621/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 7.1420 - val_loss: 2.9861\n",
      "Epoch 622/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 7.1116 - val_loss: 3.2001\n",
      "Epoch 623/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 7.1234 - val_loss: 3.2149\n",
      "Epoch 624/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 7.1791 - val_loss: 2.9988\n",
      "Epoch 625/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 7.1287 - val_loss: 3.1125\n",
      "Epoch 626/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 6.9924 - val_loss: 2.9526\n",
      "Epoch 627/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 7.0210 - val_loss: 2.9091\n",
      "Epoch 628/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 7.2332 - val_loss: 2.8983\n",
      "Epoch 629/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 7.0314 - val_loss: 3.7597\n",
      "Epoch 630/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 7.1966 - val_loss: 2.9011\n",
      "Epoch 631/1000\n",
      "382/382 [==============================] - 0s 422us/step - loss: 7.7132 - val_loss: 3.4301\n",
      "Epoch 632/1000\n",
      "382/382 [==============================] - 0s 437us/step - loss: 8.0853 - val_loss: 5.7960\n",
      "Epoch 633/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 8.6578 - val_loss: 3.0763\n",
      "Epoch 634/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 7.6078 - val_loss: 2.9219\n",
      "Epoch 635/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 7.3903 - val_loss: 4.5432\n",
      "Epoch 636/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 7.3025 - val_loss: 3.7850\n",
      "Epoch 637/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 7.5743 - val_loss: 4.6969\n",
      "Epoch 638/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 7.2063 - val_loss: 2.8359\n",
      "Epoch 639/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 6.8353 - val_loss: 2.9967\n",
      "Epoch 640/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 6.8677 - val_loss: 3.3540\n",
      "Epoch 641/1000\n",
      "382/382 [==============================] - 0s 414us/step - loss: 6.7678 - val_loss: 2.9414\n",
      "Epoch 642/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 6.9002 - val_loss: 3.1595\n",
      "Epoch 643/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 6.6284 - val_loss: 2.7837\n",
      "Epoch 644/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 6.6023 - val_loss: 2.9882\n",
      "Epoch 645/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 6.7933 - val_loss: 3.4539\n",
      "Epoch 646/1000\n",
      "382/382 [==============================] - 0s 435us/step - loss: 6.6518 - val_loss: 2.8947\n",
      "Epoch 647/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 6.7467 - val_loss: 3.5831\n",
      "Epoch 648/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 6.7718 - val_loss: 2.7618\n",
      "Epoch 649/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 6.7538 - val_loss: 2.8775\n",
      "Epoch 650/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 6.8402 - val_loss: 3.5334\n",
      "Epoch 651/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 6.7651 - val_loss: 2.8715\n",
      "Epoch 652/1000\n",
      "382/382 [==============================] - 0s 442us/step - loss: 6.6533 - val_loss: 2.6839\n",
      "Epoch 653/1000\n",
      "382/382 [==============================] - 0s 448us/step - loss: 6.5075 - val_loss: 3.6741\n",
      "Epoch 654/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 6.7250 - val_loss: 2.7395\n",
      "Epoch 655/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 6.7449 - val_loss: 2.7905\n",
      "Epoch 656/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 6.7938 - val_loss: 3.6354\n",
      "Epoch 657/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 6.5644 - val_loss: 2.8353\n",
      "Epoch 658/1000\n",
      "382/382 [==============================] - 0s 503us/step - loss: 6.7574 - val_loss: 3.7429\n",
      "Epoch 659/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 7.0353 - val_loss: 3.2821\n",
      "Epoch 660/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 6.9505 - val_loss: 2.7245\n",
      "Epoch 661/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 6.5527 - val_loss: 2.6826\n",
      "Epoch 662/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 6.3432 - val_loss: 3.4481\n",
      "Epoch 663/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 6.5894 - val_loss: 3.4070\n",
      "Epoch 664/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 7.1066 - val_loss: 3.0360\n",
      "Epoch 665/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 7.0655 - val_loss: 4.5493\n",
      "Epoch 666/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 7.8005 - val_loss: 3.5328\n",
      "Epoch 667/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 7.1527 - val_loss: 2.7715\n",
      "Epoch 668/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 6.6556 - val_loss: 3.4516\n",
      "Epoch 669/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 6.6680 - val_loss: 2.5717\n",
      "Epoch 670/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 6.1347 - val_loss: 2.6469\n",
      "Epoch 671/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 6.0677 - val_loss: 2.5356\n",
      "Epoch 672/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 6.0829 - val_loss: 3.6477\n",
      "Epoch 673/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 6.4606 - val_loss: 2.8575\n",
      "Epoch 674/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 6.4161 - val_loss: 2.7353\n",
      "Epoch 675/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 6.1302 - val_loss: 2.9024\n",
      "Epoch 676/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 6.0447 - val_loss: 2.4822\n",
      "Epoch 677/1000\n",
      "382/382 [==============================] - 0s 448us/step - loss: 5.9979 - val_loss: 3.2491\n",
      "Epoch 678/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 6.2008 - val_loss: 2.4619\n",
      "Epoch 679/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 5.9596 - val_loss: 2.8006\n",
      "Epoch 680/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 6.0640 - val_loss: 2.8709\n",
      "Epoch 681/1000\n",
      "382/382 [==============================] - 0s 467us/step - loss: 6.1759 - val_loss: 2.6481\n",
      "Epoch 682/1000\n",
      "382/382 [==============================] - 0s 418us/step - loss: 6.1970 - val_loss: 2.5039\n",
      "Epoch 683/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 6.2108 - val_loss: 3.6265\n",
      "Epoch 684/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 6.0595 - val_loss: 2.4299\n",
      "Epoch 685/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 6.0089 - val_loss: 2.3944\n",
      "Epoch 686/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 5.8338 - val_loss: 3.6208\n",
      "Epoch 687/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382/382 [==============================] - 0s 416us/step - loss: 6.2574 - val_loss: 2.5732\n",
      "Epoch 688/1000\n",
      "382/382 [==============================] - 0s 435us/step - loss: 6.7136 - val_loss: 2.7030\n",
      "Epoch 689/1000\n",
      "382/382 [==============================] - 0s 453us/step - loss: 7.2539 - val_loss: 5.4619\n",
      "Epoch 690/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 6.9141 - val_loss: 2.3691\n",
      "Epoch 691/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 6.4900 - val_loss: 2.7037\n",
      "Epoch 692/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 6.2867 - val_loss: 2.5669\n",
      "Epoch 693/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 5.6711 - val_loss: 2.3752\n",
      "Epoch 694/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 5.6940 - val_loss: 2.3258\n",
      "Epoch 695/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 5.6919 - val_loss: 2.2937\n",
      "Epoch 696/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 5.7335 - val_loss: 3.3082\n",
      "Epoch 697/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 5.8324 - val_loss: 2.4630\n",
      "Epoch 698/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 6.1414 - val_loss: 2.3056\n",
      "Epoch 699/1000\n",
      "382/382 [==============================] - 0s 440us/step - loss: 6.0535 - val_loss: 5.4721\n",
      "Epoch 700/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 7.1454 - val_loss: 3.0569\n",
      "Epoch 701/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 6.3190 - val_loss: 2.2961\n",
      "Epoch 702/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 5.5388 - val_loss: 2.2658\n",
      "Epoch 703/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 5.5060 - val_loss: 2.2603\n",
      "Epoch 704/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 5.4710 - val_loss: 2.3112\n",
      "Epoch 705/1000\n",
      "382/382 [==============================] - 0s 455us/step - loss: 5.4617 - val_loss: 2.5391\n",
      "Epoch 706/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 5.5280 - val_loss: 2.4832\n",
      "Epoch 707/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 5.6415 - val_loss: 3.0078\n",
      "Epoch 708/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 6.5295 - val_loss: 2.7168\n",
      "Epoch 709/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 6.6948 - val_loss: 4.4090\n",
      "Epoch 710/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 5.9728 - val_loss: 2.8916\n",
      "Epoch 711/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 6.4948 - val_loss: 2.2187\n",
      "Epoch 712/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 6.0098 - val_loss: 4.3627\n",
      "Epoch 713/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 6.1490 - val_loss: 2.1644\n",
      "Epoch 714/1000\n",
      "382/382 [==============================] - 0s 435us/step - loss: 5.2979 - val_loss: 2.5121\n",
      "Epoch 715/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 5.6485 - val_loss: 3.4634\n",
      "Epoch 716/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 6.3202 - val_loss: 2.3358\n",
      "Epoch 717/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 5.8755 - val_loss: 2.1303\n",
      "Epoch 718/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 5.7413 - val_loss: 2.4260\n",
      "Epoch 719/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 5.2871 - val_loss: 2.3547\n",
      "Epoch 720/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 5.1669 - val_loss: 2.1150\n",
      "Epoch 721/1000\n",
      "382/382 [==============================] - 0s 442us/step - loss: 5.3523 - val_loss: 2.3728\n",
      "Epoch 722/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 5.3502 - val_loss: 3.1797\n",
      "Epoch 723/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 5.2866 - val_loss: 2.4032\n",
      "Epoch 724/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 6.0062 - val_loss: 2.3025\n",
      "Epoch 725/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 5.5045 - val_loss: 2.9947\n",
      "Epoch 726/1000\n",
      "382/382 [==============================] - 0s 448us/step - loss: 5.7425 - val_loss: 2.5990\n",
      "Epoch 727/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 5.8923 - val_loss: 3.2438\n",
      "Epoch 728/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 6.4467 - val_loss: 2.0859\n",
      "Epoch 729/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 5.7790 - val_loss: 4.0794\n",
      "Epoch 730/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 5.9729 - val_loss: 2.4865\n",
      "Epoch 731/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 5.3570 - val_loss: 2.1210\n",
      "Epoch 732/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 5.4092 - val_loss: 2.7188\n",
      "Epoch 733/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 5.2031 - val_loss: 2.0000\n",
      "Epoch 734/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 5.3530 - val_loss: 2.2583\n",
      "Epoch 735/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 5.3687 - val_loss: 2.3581\n",
      "Epoch 736/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 4.9937 - val_loss: 2.1468\n",
      "Epoch 737/1000\n",
      "382/382 [==============================] - 0s 432us/step - loss: 5.0889 - val_loss: 2.4279\n",
      "Epoch 738/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 5.3888 - val_loss: 2.0046\n",
      "Epoch 739/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 4.9081 - val_loss: 2.7951\n",
      "Epoch 740/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 5.0343 - val_loss: 1.9376\n",
      "Epoch 741/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 5.0221 - val_loss: 2.2252\n",
      "Epoch 742/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 5.8575 - val_loss: 2.9811\n",
      "Epoch 743/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 5.3994 - val_loss: 2.5636\n",
      "Epoch 744/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 5.0715 - val_loss: 2.0167\n",
      "Epoch 745/1000\n",
      "382/382 [==============================] - 0s 450us/step - loss: 5.0236 - val_loss: 1.9812\n",
      "Epoch 746/1000\n",
      "382/382 [==============================] - 0s 432us/step - loss: 5.2599 - val_loss: 2.7090\n",
      "Epoch 747/1000\n",
      "382/382 [==============================] - 0s 435us/step - loss: 4.9108 - val_loss: 1.9511\n",
      "Epoch 748/1000\n",
      "382/382 [==============================] - 0s 437us/step - loss: 4.7356 - val_loss: 1.9280\n",
      "Epoch 749/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 4.7810 - val_loss: 1.9582\n",
      "Epoch 750/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 4.7299 - val_loss: 2.0372\n",
      "Epoch 751/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 5.3033 - val_loss: 3.0977\n",
      "Epoch 752/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 5.0465 - val_loss: 2.2871\n",
      "Epoch 753/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 4.8515 - val_loss: 1.9087\n",
      "Epoch 754/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 4.9972 - val_loss: 2.2011\n",
      "Epoch 755/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 5.4773 - val_loss: 5.2580\n",
      "Epoch 756/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 6.2935 - val_loss: 2.3897\n",
      "Epoch 757/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 5.6009 - val_loss: 3.4705\n",
      "Epoch 758/1000\n",
      "382/382 [==============================] - 0s 445us/step - loss: 5.7653 - val_loss: 2.7478\n",
      "Epoch 759/1000\n",
      "382/382 [==============================] - 0s 437us/step - loss: 4.9376 - val_loss: 2.0837\n",
      "Epoch 760/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 4.6180 - val_loss: 1.8246\n",
      "Epoch 761/1000\n",
      "382/382 [==============================] - 0s 435us/step - loss: 4.5134 - val_loss: 1.8557\n",
      "Epoch 762/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 4.4649 - val_loss: 1.8415\n",
      "Epoch 763/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 4.6269 - val_loss: 3.3322\n",
      "Epoch 764/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382/382 [==============================] - 0s 416us/step - loss: 4.8264 - val_loss: 1.7525\n",
      "Epoch 765/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 4.3393 - val_loss: 2.0437\n",
      "Epoch 766/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 4.3954 - val_loss: 1.9128\n",
      "Epoch 767/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 4.5982 - val_loss: 1.7649\n",
      "Epoch 768/1000\n",
      "382/382 [==============================] - 0s 476us/step - loss: 4.6591 - val_loss: 1.9333\n",
      "Epoch 769/1000\n",
      "382/382 [==============================] - 0s 448us/step - loss: 4.6944 - val_loss: 1.9967\n",
      "Epoch 770/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 4.3874 - val_loss: 2.1687\n",
      "Epoch 771/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 4.3510 - val_loss: 1.7381\n",
      "Epoch 772/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 4.2360 - val_loss: 1.9942\n",
      "Epoch 773/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 4.5265 - val_loss: 2.4313\n",
      "Epoch 774/1000\n",
      "382/382 [==============================] - 0s 453us/step - loss: 4.5572 - val_loss: 2.3158\n",
      "Epoch 775/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 4.8165 - val_loss: 1.8061\n",
      "Epoch 776/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 4.6370 - val_loss: 1.9262\n",
      "Epoch 777/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 4.5401 - val_loss: 1.9360\n",
      "Epoch 778/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 4.2323 - val_loss: 1.8398\n",
      "Epoch 779/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 4.2892 - val_loss: 1.6309\n",
      "Epoch 780/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 4.1765 - val_loss: 1.6208\n",
      "Epoch 781/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 4.4921 - val_loss: 1.6217\n",
      "Epoch 782/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 4.3951 - val_loss: 2.6548\n",
      "Epoch 783/1000\n",
      "382/382 [==============================] - 0s 435us/step - loss: 4.2195 - val_loss: 1.6274\n",
      "Epoch 784/1000\n",
      "382/382 [==============================] - 0s 437us/step - loss: 4.3569 - val_loss: 1.7674\n",
      "Epoch 785/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 4.2630 - val_loss: 2.1819\n",
      "Epoch 786/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 4.4933 - val_loss: 1.7583\n",
      "Epoch 787/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 4.4641 - val_loss: 3.0457\n",
      "Epoch 788/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 4.5486 - val_loss: 1.5937\n",
      "Epoch 789/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 4.2737 - val_loss: 1.7411\n",
      "Epoch 790/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 4.3382 - val_loss: 1.5789\n",
      "Epoch 791/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 4.3670 - val_loss: 1.5451\n",
      "Epoch 792/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 4.1439 - val_loss: 1.9057\n",
      "Epoch 793/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 3.9682 - val_loss: 1.8011\n",
      "Epoch 794/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 3.9550 - val_loss: 1.5363\n",
      "Epoch 795/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 3.9367 - val_loss: 2.1078\n",
      "Epoch 796/1000\n",
      "382/382 [==============================] - 0s 432us/step - loss: 4.0509 - val_loss: 2.2076\n",
      "Epoch 797/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 4.3376 - val_loss: 1.5224\n",
      "Epoch 798/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 4.0668 - val_loss: 1.9556\n",
      "Epoch 799/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 4.4794 - val_loss: 1.9388\n",
      "Epoch 800/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 4.1375 - val_loss: 2.9833\n",
      "Epoch 801/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 4.0733 - val_loss: 1.7447\n",
      "Epoch 802/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 4.1538 - val_loss: 1.6725\n",
      "Epoch 803/1000\n",
      "382/382 [==============================] - 0s 463us/step - loss: 4.2334 - val_loss: 2.2319\n",
      "Epoch 804/1000\n",
      "382/382 [==============================] - 0s 432us/step - loss: 4.2780 - val_loss: 1.7502\n",
      "Epoch 805/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 3.7486 - val_loss: 1.8273\n",
      "Epoch 806/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 3.7868 - val_loss: 1.4971\n",
      "Epoch 807/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 4.5129 - val_loss: 3.7122\n",
      "Epoch 808/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 5.6255 - val_loss: 2.8594\n",
      "Epoch 809/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 4.7653 - val_loss: 2.3037\n",
      "Epoch 810/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 5.0835 - val_loss: 2.8998\n",
      "Epoch 811/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 4.9076 - val_loss: 1.5587\n",
      "Epoch 812/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 4.1271 - val_loss: 2.8206\n",
      "Epoch 813/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 4.7191 - val_loss: 1.7033\n",
      "Epoch 814/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 4.4479 - val_loss: 2.5840\n",
      "Epoch 815/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 4.5471 - val_loss: 1.3919\n",
      "Epoch 816/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 4.2125 - val_loss: 2.7043\n",
      "Epoch 817/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 3.9713 - val_loss: 2.1177\n",
      "Epoch 818/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 3.8330 - val_loss: 1.4391\n",
      "Epoch 819/1000\n",
      "382/382 [==============================] - 0s 432us/step - loss: 3.6644 - val_loss: 1.3675\n",
      "Epoch 820/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 3.7871 - val_loss: 1.9196\n",
      "Epoch 821/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 3.8416 - val_loss: 2.3838\n",
      "Epoch 822/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 3.7112 - val_loss: 1.5180\n",
      "Epoch 823/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 3.4064 - val_loss: 1.5736\n",
      "Epoch 824/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 3.5062 - val_loss: 1.9971\n",
      "Epoch 825/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 3.3830 - val_loss: 1.2996\n",
      "Epoch 826/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 3.3632 - val_loss: 1.3181\n",
      "Epoch 827/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 3.3343 - val_loss: 1.3963\n",
      "Epoch 828/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 3.5746 - val_loss: 1.2711\n",
      "Epoch 829/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 3.4013 - val_loss: 1.8117\n",
      "Epoch 830/1000\n",
      "382/382 [==============================] - 0s 419us/step - loss: 3.3284 - val_loss: 1.3111\n",
      "Epoch 831/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 3.3338 - val_loss: 1.2577\n",
      "Epoch 832/1000\n",
      "382/382 [==============================] - 0s 453us/step - loss: 3.2711 - val_loss: 1.2551\n",
      "Epoch 833/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 3.3590 - val_loss: 1.8124\n",
      "Epoch 834/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 3.4180 - val_loss: 2.1200\n",
      "Epoch 835/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 3.6657 - val_loss: 1.4014\n",
      "Epoch 836/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 3.5291 - val_loss: 1.4141\n",
      "Epoch 837/1000\n",
      "382/382 [==============================] - 0s 453us/step - loss: 3.4189 - val_loss: 1.2330\n",
      "Epoch 838/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 3.4240 - val_loss: 1.2400\n",
      "Epoch 839/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 3.6148 - val_loss: 1.7711\n",
      "Epoch 840/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 3.6033 - val_loss: 3.5151\n",
      "Epoch 841/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382/382 [==============================] - 0s 427us/step - loss: 4.3369 - val_loss: 2.7409\n",
      "Epoch 842/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 3.7548 - val_loss: 1.3213\n",
      "Epoch 843/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 3.4176 - val_loss: 1.4057\n",
      "Epoch 844/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 3.4177 - val_loss: 1.1727\n",
      "Epoch 845/1000\n",
      "382/382 [==============================] - 0s 450us/step - loss: 3.1902 - val_loss: 1.3175\n",
      "Epoch 846/1000\n",
      "382/382 [==============================] - 0s 435us/step - loss: 3.1142 - val_loss: 1.7427\n",
      "Epoch 847/1000\n",
      "382/382 [==============================] - 0s 440us/step - loss: 3.3607 - val_loss: 2.8485\n",
      "Epoch 848/1000\n",
      "382/382 [==============================] - 0s 435us/step - loss: 3.6992 - val_loss: 1.1424\n",
      "Epoch 849/1000\n",
      "382/382 [==============================] - 0s 437us/step - loss: 3.1793 - val_loss: 1.5709\n",
      "Epoch 850/1000\n",
      "382/382 [==============================] - 0s 435us/step - loss: 3.5835 - val_loss: 1.1353\n",
      "Epoch 851/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 3.2604 - val_loss: 3.1906\n",
      "Epoch 852/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 3.8072 - val_loss: 2.1368\n",
      "Epoch 853/1000\n",
      "382/382 [==============================] - 0s 461us/step - loss: 3.5287 - val_loss: 1.4032\n",
      "Epoch 854/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 3.4176 - val_loss: 1.2129\n",
      "Epoch 855/1000\n",
      "382/382 [==============================] - 0s 437us/step - loss: 3.4420 - val_loss: 1.6349\n",
      "Epoch 856/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 3.3171 - val_loss: 1.1248\n",
      "Epoch 857/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 3.0291 - val_loss: 1.7166\n",
      "Epoch 858/1000\n",
      "382/382 [==============================] - 0s 440us/step - loss: 2.9407 - val_loss: 1.1502\n",
      "Epoch 859/1000\n",
      "382/382 [==============================] - 0s 435us/step - loss: 3.2605 - val_loss: 1.1874\n",
      "Epoch 860/1000\n",
      "382/382 [==============================] - 0s 435us/step - loss: 3.1244 - val_loss: 1.4381\n",
      "Epoch 861/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 3.1068 - val_loss: 1.0865\n",
      "Epoch 862/1000\n",
      "382/382 [==============================] - 0s 448us/step - loss: 3.2209 - val_loss: 1.1742\n",
      "Epoch 863/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 3.0802 - val_loss: 1.2421\n",
      "Epoch 864/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 2.7695 - val_loss: 1.2543\n",
      "Epoch 865/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 2.9232 - val_loss: 2.1347\n",
      "Epoch 866/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 3.1135 - val_loss: 1.1524\n",
      "Epoch 867/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 2.7190 - val_loss: 1.0225\n",
      "Epoch 868/1000\n",
      "382/382 [==============================] - 0s 435us/step - loss: 2.7107 - val_loss: 1.0811\n",
      "Epoch 869/1000\n",
      "382/382 [==============================] - 0s 445us/step - loss: 2.6980 - val_loss: 1.0932\n",
      "Epoch 870/1000\n",
      "382/382 [==============================] - 0s 445us/step - loss: 2.7642 - val_loss: 1.1836\n",
      "Epoch 871/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 2.8070 - val_loss: 1.0413\n",
      "Epoch 872/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 2.6370 - val_loss: 1.0404\n",
      "Epoch 873/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 2.6806 - val_loss: 1.7863\n",
      "Epoch 874/1000\n",
      "382/382 [==============================] - 0s 432us/step - loss: 2.8698 - val_loss: 2.0106\n",
      "Epoch 875/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 3.2011 - val_loss: 1.2847\n",
      "Epoch 876/1000\n",
      "382/382 [==============================] - 0s 445us/step - loss: 3.4792 - val_loss: 1.0551\n",
      "Epoch 877/1000\n",
      "382/382 [==============================] - 0s 435us/step - loss: 3.2925 - val_loss: 1.1921\n",
      "Epoch 878/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 2.8321 - val_loss: 0.9621\n",
      "Epoch 879/1000\n",
      "382/382 [==============================] - 0s 437us/step - loss: 2.5812 - val_loss: 0.9602\n",
      "Epoch 880/1000\n",
      "382/382 [==============================] - 0s 437us/step - loss: 2.5280 - val_loss: 1.3017\n",
      "Epoch 881/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 2.6218 - val_loss: 1.5954\n",
      "Epoch 882/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 2.7002 - val_loss: 0.9985\n",
      "Epoch 883/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 2.6062 - val_loss: 0.9232\n",
      "Epoch 884/1000\n",
      "382/382 [==============================] - 0s 432us/step - loss: 2.8280 - val_loss: 0.9689\n",
      "Epoch 885/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 2.8887 - val_loss: 1.5358\n",
      "Epoch 886/1000\n",
      "382/382 [==============================] - 0s 432us/step - loss: 3.0362 - val_loss: 1.6456\n",
      "Epoch 887/1000\n",
      "382/382 [==============================] - 0s 437us/step - loss: 3.2997 - val_loss: 1.1638\n",
      "Epoch 888/1000\n",
      "382/382 [==============================] - 0s 442us/step - loss: 3.0899 - val_loss: 1.8805\n",
      "Epoch 889/1000\n",
      "382/382 [==============================] - 0s 435us/step - loss: 2.7904 - val_loss: 2.2798\n",
      "Epoch 890/1000\n",
      "382/382 [==============================] - 0s 435us/step - loss: 2.6885 - val_loss: 0.9243\n",
      "Epoch 891/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 2.5126 - val_loss: 1.1245\n",
      "Epoch 892/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 2.6767 - val_loss: 0.9912\n",
      "Epoch 893/1000\n",
      "382/382 [==============================] - 0s 435us/step - loss: 2.6300 - val_loss: 1.2026\n",
      "Epoch 894/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 2.5313 - val_loss: 0.9731\n",
      "Epoch 895/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 2.3916 - val_loss: 2.1858\n",
      "Epoch 896/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 2.8502 - val_loss: 1.8128\n",
      "Epoch 897/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 2.5854 - val_loss: 1.0864\n",
      "Epoch 898/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 2.3579 - val_loss: 0.8453\n",
      "Epoch 899/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 2.4545 - val_loss: 1.1872\n",
      "Epoch 900/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 2.7032 - val_loss: 1.2792\n",
      "Epoch 901/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 2.7978 - val_loss: 1.8221\n",
      "Epoch 902/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 3.1862 - val_loss: 1.2680\n",
      "Epoch 903/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 2.9621 - val_loss: 0.8858\n",
      "Epoch 904/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 2.6627 - val_loss: 2.9803\n",
      "Epoch 905/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 2.7888 - val_loss: 1.4831\n",
      "Epoch 906/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 2.3278 - val_loss: 0.8389\n",
      "Epoch 907/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 2.1931 - val_loss: 0.8549\n",
      "Epoch 908/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 2.1305 - val_loss: 0.7901\n",
      "Epoch 909/1000\n",
      "382/382 [==============================] - 0s 445us/step - loss: 2.2385 - val_loss: 0.7833\n",
      "Epoch 910/1000\n",
      "382/382 [==============================] - 0s 442us/step - loss: 2.2966 - val_loss: 1.1780\n",
      "Epoch 911/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 2.3977 - val_loss: 0.7978\n",
      "Epoch 912/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 2.1108 - val_loss: 0.7778\n",
      "Epoch 913/1000\n",
      "382/382 [==============================] - 0s 432us/step - loss: 2.1899 - val_loss: 0.7775\n",
      "Epoch 914/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 2.1934 - val_loss: 0.7567\n",
      "Epoch 915/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 2.0544 - val_loss: 0.8875\n",
      "Epoch 916/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 2.1899 - val_loss: 1.5929\n",
      "Epoch 917/1000\n",
      "382/382 [==============================] - 0s 453us/step - loss: 2.7595 - val_loss: 0.7481\n",
      "Epoch 918/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382/382 [==============================] - 0s 453us/step - loss: 3.1152 - val_loss: 1.4877\n",
      "Epoch 919/1000\n",
      "382/382 [==============================] - 0s 445us/step - loss: 2.7473 - val_loss: 2.3613\n",
      "Epoch 920/1000\n",
      "382/382 [==============================] - 0s 440us/step - loss: 3.2010 - val_loss: 1.0025\n",
      "Epoch 921/1000\n",
      "382/382 [==============================] - 0s 432us/step - loss: 2.8438 - val_loss: 0.8183\n",
      "Epoch 922/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 2.5032 - val_loss: 1.4376\n",
      "Epoch 923/1000\n",
      "382/382 [==============================] - 0s 422us/step - loss: 2.6633 - val_loss: 2.9795\n",
      "Epoch 924/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 2.7173 - val_loss: 1.7670\n",
      "Epoch 925/1000\n",
      "382/382 [==============================] - 0s 435us/step - loss: 2.2457 - val_loss: 0.9285\n",
      "Epoch 926/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 1.9464 - val_loss: 0.8158\n",
      "Epoch 927/1000\n",
      "382/382 [==============================] - 0s 458us/step - loss: 1.9615 - val_loss: 1.0107\n",
      "Epoch 928/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 1.9117 - val_loss: 1.0118\n",
      "Epoch 929/1000\n",
      "382/382 [==============================] - 0s 435us/step - loss: 1.8926 - val_loss: 0.7406\n",
      "Epoch 930/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 2.2523 - val_loss: 1.0293\n",
      "Epoch 931/1000\n",
      "382/382 [==============================] - 0s 458us/step - loss: 2.0909 - val_loss: 0.7690\n",
      "Epoch 932/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 2.0760 - val_loss: 0.7207\n",
      "Epoch 933/1000\n",
      "382/382 [==============================] - 0s 432us/step - loss: 1.9866 - val_loss: 0.7770\n",
      "Epoch 934/1000\n",
      "382/382 [==============================] - 0s 432us/step - loss: 1.9361 - val_loss: 0.6673\n",
      "Epoch 935/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 1.9791 - val_loss: 0.7013\n",
      "Epoch 936/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 1.8905 - val_loss: 1.1519\n",
      "Epoch 937/1000\n",
      "382/382 [==============================] - 0s 435us/step - loss: 2.0787 - val_loss: 0.7287\n",
      "Epoch 938/1000\n",
      "382/382 [==============================] - 0s 442us/step - loss: 1.8471 - val_loss: 0.8606\n",
      "Epoch 939/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 1.7669 - val_loss: 0.8336\n",
      "Epoch 940/1000\n",
      "382/382 [==============================] - 0s 445us/step - loss: 1.7738 - val_loss: 0.6769\n",
      "Epoch 941/1000\n",
      "382/382 [==============================] - 0s 435us/step - loss: 1.7451 - val_loss: 0.6975\n",
      "Epoch 942/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 1.6923 - val_loss: 1.3995\n",
      "Epoch 943/1000\n",
      "382/382 [==============================] - 0s 442us/step - loss: 2.1167 - val_loss: 2.2908\n",
      "Epoch 944/1000\n",
      "382/382 [==============================] - 0s 440us/step - loss: 2.1365 - val_loss: 1.7940\n",
      "Epoch 945/1000\n",
      "382/382 [==============================] - 0s 432us/step - loss: 1.9248 - val_loss: 0.8027\n",
      "Epoch 946/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 1.7935 - val_loss: 0.9358\n",
      "Epoch 947/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 1.6629 - val_loss: 0.6089\n",
      "Epoch 948/1000\n",
      "382/382 [==============================] - 0s 445us/step - loss: 1.7719 - val_loss: 0.6430\n",
      "Epoch 949/1000\n",
      "382/382 [==============================] - 0s 442us/step - loss: 1.9028 - val_loss: 0.5986\n",
      "Epoch 950/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 1.7242 - val_loss: 0.6429\n",
      "Epoch 951/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 1.6816 - val_loss: 0.7457\n",
      "Epoch 952/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 1.7710 - val_loss: 0.6629\n",
      "Epoch 953/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 1.6514 - val_loss: 0.5895\n",
      "Epoch 954/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 1.6508 - val_loss: 0.7326\n",
      "Epoch 955/1000\n",
      "382/382 [==============================] - 0s 432us/step - loss: 1.6232 - val_loss: 0.5793\n",
      "Epoch 956/1000\n",
      "382/382 [==============================] - 0s 432us/step - loss: 1.6823 - val_loss: 0.7579\n",
      "Epoch 957/1000\n",
      "382/382 [==============================] - 0s 445us/step - loss: 1.8991 - val_loss: 0.9025\n",
      "Epoch 958/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 1.7689 - val_loss: 0.6526\n",
      "Epoch 959/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 1.9099 - val_loss: 0.7829\n",
      "Epoch 960/1000\n",
      "382/382 [==============================] - 0s 432us/step - loss: 1.9382 - val_loss: 0.8382\n",
      "Epoch 961/1000\n",
      "382/382 [==============================] - 0s 435us/step - loss: 2.2659 - val_loss: 0.5686\n",
      "Epoch 962/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 2.2474 - val_loss: 0.9305\n",
      "Epoch 963/1000\n",
      "382/382 [==============================] - 0s 453us/step - loss: 2.0660 - val_loss: 1.9166\n",
      "Epoch 964/1000\n",
      "382/382 [==============================] - 0s 466us/step - loss: 2.2918 - val_loss: 3.8455\n",
      "Epoch 965/1000\n",
      "382/382 [==============================] - 0s 474us/step - loss: 3.0237 - val_loss: 1.2604\n",
      "Epoch 966/1000\n",
      "382/382 [==============================] - 0s 437us/step - loss: 1.7945 - val_loss: 0.6143\n",
      "Epoch 967/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 1.7720 - val_loss: 1.4672\n",
      "Epoch 968/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 1.7325 - val_loss: 0.7745\n",
      "Epoch 969/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 1.4914 - val_loss: 0.7538\n",
      "Epoch 970/1000\n",
      "382/382 [==============================] - 0s 435us/step - loss: 1.4491 - val_loss: 0.6744\n",
      "Epoch 971/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 1.7066 - val_loss: 0.6531\n",
      "Epoch 972/1000\n",
      "382/382 [==============================] - 0s 421us/step - loss: 1.4999 - val_loss: 0.5313\n",
      "Epoch 973/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 1.4306 - val_loss: 0.5272\n",
      "Epoch 974/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 1.5727 - val_loss: 0.6796\n",
      "Epoch 975/1000\n",
      "382/382 [==============================] - 0s 473us/step - loss: 1.7321 - val_loss: 0.6642\n",
      "Epoch 976/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 1.9901 - val_loss: 0.5238\n",
      "Epoch 977/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 2.1223 - val_loss: 0.5519\n",
      "Epoch 978/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 1.6633 - val_loss: 0.5557\n",
      "Epoch 979/1000\n",
      "382/382 [==============================] - 0s 432us/step - loss: 1.5644 - val_loss: 0.6165\n",
      "Epoch 980/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 1.5649 - val_loss: 0.5111\n",
      "Epoch 981/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 1.6964 - val_loss: 0.6829\n",
      "Epoch 982/1000\n",
      "382/382 [==============================] - 0s 432us/step - loss: 1.5455 - val_loss: 1.7374\n",
      "Epoch 983/1000\n",
      "382/382 [==============================] - 0s 440us/step - loss: 1.6278 - val_loss: 1.0187\n",
      "Epoch 984/1000\n",
      "382/382 [==============================] - 0s 445us/step - loss: 1.3951 - val_loss: 0.5156\n",
      "Epoch 985/1000\n",
      "382/382 [==============================] - 0s 463us/step - loss: 1.4211 - val_loss: 0.4991\n",
      "Epoch 986/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 1.3268 - val_loss: 0.6808\n",
      "Epoch 987/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 1.2945 - val_loss: 0.5050\n",
      "Epoch 988/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 1.2855 - val_loss: 0.5535\n",
      "Epoch 989/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 1.3191 - val_loss: 0.5003\n",
      "Epoch 990/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 1.2914 - val_loss: 0.4899\n",
      "Epoch 991/1000\n",
      "382/382 [==============================] - 0s 429us/step - loss: 1.2560 - val_loss: 0.4875\n",
      "Epoch 992/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 1.3543 - val_loss: 0.4810\n",
      "Epoch 993/1000\n",
      "382/382 [==============================] - 0s 424us/step - loss: 1.2804 - val_loss: 0.5256\n",
      "Epoch 994/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 1.3566 - val_loss: 0.5867\n",
      "Epoch 995/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382/382 [==============================] - 0s 429us/step - loss: 1.4762 - val_loss: 0.5263\n",
      "Epoch 996/1000\n",
      "382/382 [==============================] - 0s 432us/step - loss: 1.3858 - val_loss: 0.5096\n",
      "Epoch 997/1000\n",
      "382/382 [==============================] - 0s 427us/step - loss: 1.3970 - val_loss: 0.5948\n",
      "Epoch 998/1000\n",
      "382/382 [==============================] - 0s 416us/step - loss: 1.2155 - val_loss: 0.5691\n",
      "Epoch 999/1000\n",
      "382/382 [==============================] - 0s 437us/step - loss: 1.1812 - val_loss: 0.7518\n",
      "Epoch 1000/1000\n",
      "382/382 [==============================] - 0s 435us/step - loss: 1.1938 - val_loss: 0.4685\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "history=network.fit(X_train,y_train, epochs=1000, batch_size=100,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 0s 1ms/step\n",
      "425/425 [==============================] - 0s 254us/step\n"
     ]
    }
   ],
   "source": [
    "predictions_test = network.predict(X_test, batch_size=5, verbose=1)\n",
    "predictions_train = network.predict(X_train, batch_size=32, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_test_score_values=np.absolute(np.subtract(y_test.astype('float32'),np.squeeze(predictions_test)))\n",
    "prediction_train_score_values=np.absolute(np.subtract(y_train.astype('float32'),np.squeeze(predictions_train)))\n",
    "prediction_test_score= np.average(prediction_test_score_values) \n",
    "prediction_train_score= np.average(prediction_train_score_values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test score: 0.658369\n",
      "train score: 0.675676\n"
     ]
    }
   ],
   "source": [
    "print ('test score:',prediction_test_score)\n",
    "print ('train score:',prediction_train_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['116.918449', '150.163681', '154.775711', '153.686325',\n",
       "       '213.378265', '202.255936', '207.070267', '136.91156', '115.316208',\n",
       "       '127.402061', '126.168449', '121.37365', '219.522171', '163.990295',\n",
       "       '190.345947', '205.153183', '169.849365', '148.630539', '139.83989',\n",
       "       '152.103088', '154.888229', '131.925919', '155.422791',\n",
       "       '117.021988', '114.126945', '186.060196', '194.562592',\n",
       "       '160.501678', '112.981071', '188.885025', '134.224533',\n",
       "       '169.064697', '131.15126', '116.974983', '122.580284', '109.834557',\n",
       "       '189.950439', '189.537811', '107.62912', '153.297653', '125.436012',\n",
       "       '208.983505', '116.837845', '162.380127', '166.432617',\n",
       "       '192.736649', '215.419373', '108.086983'], \n",
       "      dtype='<U32')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = pd.DataFrame(data={'original':y_test.reshape((len(y_test),)),'prediction':predictions_test.reshape((len(y_test),))})\n",
    "compare.to_csv('C:/git/cropped/compare-test'+strftime(\"%H_%M_%S\", gmtime())+'.csv')\n",
    "compare = pd.DataFrame(data={'original':y_train.reshape((len(X_train),)),'prediction':predictions_train.reshape((len(X_train),))})\n",
    "compare.to_csv('C:/git/cropped/compare-train'+strftime(\"%H_%M_%S\", gmtime())+'.csv');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19121889908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "h=prediction_train_score_values\n",
    "#fit = stats.norm.pdf(h, np.mean(h), np.std(h))  #this is a fitting indeed\n",
    "#plt.plot(h,'-o')\n",
    "plt.hist(h,normed=True)      #use this to draw histogram of your data\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    }
   ],
   "source": [
    "# Reading json data from url\n",
    "import requests\n",
    "import pandas\n",
    "dataJson = requests.get(\"https://34.240.232.82/api/PrintbeatService/whisperContent/lastRequests\"\n",
    "                  ,verify=False).json()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'element': '1',\n",
       " 'id': 347,\n",
       " 'phone_model': 'Samsung SM-G930F',\n",
       " 'result_url': 'https://34.240.232.82/results_sample_2018-03-29T13:02:45.283Z_1__testing_app._ignore._.txt',\n",
       " 'test_reason': 'system ok, testing application ',\n",
       " 'test_reason_comments': '_testing app. ignore.',\n",
       " 'test_result_comments': None,\n",
       " 'time': '2018-03-29T13:02:52.000Z',\n",
       " 'wave_url': 'https://34.240.232.82/sample_2018-03-29T13:02:45.283Z_1__testing_app._ignore._.wav'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing first element\n",
    "\n",
    "dataJson[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://34.240.232.82/sample_2018-03-29T13:02:45.283Z_1__testing_app._ignore._.wav'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing first record wav url \n",
    "\n",
    "dataJson[0]['wave_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving wav file \n",
    "\n",
    "import urllib\n",
    "import ssl\n",
    "\n",
    "context = ssl._create_unverified_context()\n",
    "\n",
    "f = urllib.request.urlopen(dataJson[0]['wave_url'], context=context)\n",
    "dataWav = f.read()\n",
    "with open(\"C:/git/cropped/table22.mp3\", \"wb\") as code:\n",
    "    code.write(dataWav)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving data into csv file \n",
    "import json\n",
    "import csv\n",
    "fieldnames = ['id', 'time','result_url', 'wave_url', 'element', 'phone_model', 'test_reason', 'test_result_comments', 'test_reason_comments']\n",
    "csvfile=open('C:/git/cropped/table32.csv', 'w')\n",
    "writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "writer.writeheader()\n",
    "for datain in dataJson:\n",
    "    writer.writerow(datain)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
